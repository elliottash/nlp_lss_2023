{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "liable-focus",
   "metadata": {},
   "source": [
    "# Week 9: Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "republican-encyclopedia",
   "metadata": {},
   "source": [
    "Additional references: \n",
    "- [Character Level Language Model (GPU required)](https://github.com/m2dsupsdlclass/lectures-labs/blob/master/labs/06_deep_nlp/Character_Level_Language_Model_rendered.ipynb)\n",
    "- [Transformers (BERT fine-tuning): Joint Intent Classification and Slot Filling](https://github.com/m2dsupsdlclass/lectures-labs/blob/master/labs/06_deep_nlp/Transformers_Joint_Intent_Classification_Slot_Filling_rendered.ipynb)\n",
    "- [Generating Language with huggingface](https://huggingface.co/blog/how-to-generate)\n",
    "- [huggingface examples](https://huggingface.co/transformers/quickstart.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "intermediate-operations",
   "metadata": {},
   "outputs": [],
   "source": [
    "#setup\n",
    "import warnings; warnings.simplefilter('ignore')\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_pickle('sc_cases_cleaned.pkl', compression='gzip')\n",
    "df = df.assign(author_id=(df['authorship']).astype('category').cat.codes)\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hollow-saskatchewan",
   "metadata": {},
   "source": [
    "## Huggingface Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "successful-elite",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import DistilBertTokenizerFast, DistilBertForSequenceClassification\n",
    "\n",
    "# gpu or cpu?\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print (device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "continuous-grant",
   "metadata": {},
   "source": [
    "Load the model from a pretrained checkpoint. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "reported-stone",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'distilbert-base-uncased' # huggingface model_ID or path to folder \n",
    "model = DistilBertForSequenceClassification.from_pretrained(model_name)\n",
    "print (model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "premium-monitor",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = DistilBertTokenizerFast.from_pretrained(model_name)\n",
    "inputs = tokenizer(df.iloc[0]['opinion_text'], return_tensors=\"pt\")\n",
    "print(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "particular-desperate",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer(df['opinion_text'].tolist(), return_tensors=\"pt\", padding=True, truncation=True)\n",
    "labels = torch.tensor(df['x_republican'].tolist()).long() \n",
    "print(inputs, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "authentic-coaching",
   "metadata": {},
   "source": [
    "More infos about huggingface tokenizers can be found [here](https://huggingface.co/transformers/main_classes/tokenizer.html).\n",
    "\n",
    "Now we have a set of text inputs and authors indicators as labels and we can train a transformers model using a cross-entropy loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "outstanding-tolerance",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_labels, counts = np.unique(df[\"x_republican\"], return_counts=True)\n",
    "print (unique_labels, counts)\n",
    "model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=len(unique_labels))\n",
    "\n",
    "optimizer = torch.optim.Adam([\n",
    "    {'params': model.distilbert.parameters(), 'lr': 1e-5},  \n",
    "    {'params': model.classifier.parameters(), 'lr': 1e-3}\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "medium-bangkok",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['opinion_text'].tolist(), df['x_republican'].tolist(), test_size=.2)\n",
    "\n",
    "# generate batches\n",
    "X_train, X_test, y_train, y_test = np.array(X_train[:608]), np.array(X_test[:152]), np.array(y_train[:608]), np.array(y_test[:152])\n",
    "print (X_train.shape, X_test.shape, y_train.shape, y_test.shape)\n",
    "\n",
    "X_train, X_test, y_train, y_test = X_train.reshape(-1, 8), X_test.reshape(-1, 8), y_train.reshape(-1, 8), y_test.reshape(-1, 8)\n",
    "print (X_train.shape, X_test.shape, y_train.shape, y_test.shape)\n",
    "\n",
    "X_train, X_test = X_train.tolist(), X_test.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "satisfactory-shakespeare",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train\n",
    "from tqdm import tqdm\n",
    "\n",
    "num_epochs = 1\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for text, labels in tqdm(zip(X_train, y_train), total=len(X_train)):\n",
    "        # prepare model input through our tokenizer\n",
    "        model_inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=256)\n",
    "        # place everything on the right device\n",
    "        model_inputs = {k:v.to(device) for k,v in model_inputs.items()}\n",
    "        # labels have to be torch long tensors\n",
    "        labels = torch.tensor(labels).long()\n",
    "        # now, we can perform the forward pass\n",
    "        output = model(**model_inputs, labels=labels)\n",
    "        loss, logits = output[:2]\n",
    "        # and the backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "scenic-noise",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions, targets = [], []\n",
    "model.eval()\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    for text, labels in tqdm(zip(X_test, y_test), total=len(X_test)):\n",
    "        model_inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "        model_inputs = {k:v.to(device) for k,v in model_inputs.items()}\n",
    "\n",
    "        output = model(**model_inputs)\n",
    "        logits = output[0]\n",
    "        # prediction is the argmax of the logits\n",
    "        predictions.extend(logits.argmax(dim=1).tolist())\n",
    "        targets.extend(labels)\n",
    "        \n",
    "from sklearn import metrics\n",
    "accuracy = metrics.accuracy_score(targets, predictions)\n",
    "print (\"accuracy\", accuracy)\n",
    "classification_report = metrics.classification_report(targets, predictions)\n",
    "print (classification_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "together-loading",
   "metadata": {},
   "source": [
    "So far, we considered the pytorch version for transformers. It also works with keras, a more in-depth tutorial can be found [here](https://towardsdatascience.com/working-with-hugging-face-transformers-and-tf-2-0-89bf35e3555a)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vanilla-order",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "killing-muslim",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TFDistilBertForSequenceClassification, DistilBertConfig\n",
    "import tensorflow as tf\n",
    "\n",
    "# note that we use TFDistilBert... instead of DistilBert...\n",
    "\n",
    "transformer_model = TFDistilBertForSequenceClassification.from_pretrained(model_name)\n",
    "\n",
    "# define model input layer\n",
    "\n",
    "input_ids = tf.keras.layers.Input(shape=(256,), name='input_token', dtype='int32')\n",
    "input_masks_ids = tf.keras.layers.Input(shape=(256,), name='masked_token', dtype='int32')\n",
    "X = transformer_model(input_ids, input_masks_ids)\n",
    "model = tf.keras.Model(inputs=[input_ids, input_masks_ids], outputs = X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "impressive-lightning",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "legendary-homeless",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='sparse_categorical_crossentropy', # cost function\n",
    "              optimizer='adam', # use adam as the optimizer\n",
    "              metrics=['accuracy']) # compute accuracy, for scoring\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "temporal-addiction",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize X_train\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['opinion_text'].tolist(), df['x_republican'].tolist(), test_size=.2)\n",
    "X_train_tf = [tokenizer(x, return_tensors=\"tf\", padding=True, truncation=True, max_length=256) for x in X_train]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "indirect-volleyball",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids, input_masks = [x[\"input_ids\"][0].numpy() for x in X_train_tf], [x[\"attention_mask\"][0].numpy() for x in X_train_tf]\n",
    "dataset = tf.data.Dataset.from_tensor_slices(({'input_token': input_ids, 'masked_token': input_masks}, y_train)).batch(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "centered-clinton",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_info = model.fit(dataset,epochs=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0328d6b9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-28T13:51:19.545240Z",
     "start_time": "2022-03-28T13:51:19.525343Z"
    }
   },
   "source": [
    "# LSTM in keras\n",
    "\n",
    "Because we have an embedding lookup now, we can train an LSTM.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac126a45",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-28T13:51:34.581751Z",
     "start_time": "2022-03-28T13:51:32.205589Z"
    }
   },
   "outputs": [],
   "source": [
    "from keras.layers import LSTM, Dense\n",
    "\n",
    "model = Sequential() # create a sequential model\n",
    "model.add(Embedding(length_vocab, 32, input_length=max_seq_length, name=\"embedding_layer\"))\n",
    "model.add(LSTM(32))\n",
    "model.add(Dense(32, activation=\"relu\"))\n",
    "model.add(Dense(1, activation=\"sigmoid\")) # output layer\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9be2cde8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-28T13:51:37.808967Z",
     "start_time": "2022-03-28T13:51:37.784205Z"
    }
   },
   "outputs": [],
   "source": [
    "from IPython.display import SVG\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "dot = model_to_dot(model,\n",
    "                   show_shapes=True,\n",
    "                   show_layer_names=False,\n",
    "                   dpi=70)\n",
    "SVG(dot.create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "995144d0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-28T13:51:43.197138Z",
     "start_time": "2022-03-28T13:51:43.183836Z"
    }
   },
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "def r2(y_true, y_pred):\n",
    "    SS_res =  K.sum(K.square( y_true-y_pred )) \n",
    "    SS_tot = K.sum(K.square( y_true - K.mean(y_true) ) ) \n",
    "    return ( 1 - SS_res/(SS_tot + K.epsilon()) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fdbc99d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-28T13:51:48.754183Z",
     "start_time": "2022-03-28T13:51:48.536374Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer(min_df=0.01, # at min 1% of docs\n",
    "                        max_df=.9,  \n",
    "                        max_features=1000,\n",
    "                        stop_words='english',\n",
    "                        ngram_range=(1,3))\n",
    "\n",
    "X = vectorizer.fit_transform(df['opinion_text'])\n",
    "Y = df['x_republican']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f38db83",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-28T13:51:54.328603Z",
     "start_time": "2022-03-28T13:51:54.305200Z"
    }
   },
   "outputs": [],
   "source": [
    "# fit the model\n",
    "model.compile(loss='mean_squared_error', # cost function\n",
    "              optimizer='adam', # use adam as the optimizer\n",
    "              metrics=[r2]) # compute accuracy, for scoring\n",
    "\n",
    "model_info = model.fit(X.todense(), X.todense(), \n",
    "                      epochs=10,\n",
    "                      validation_split=.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb7c1164",
   "metadata": {},
   "source": [
    "**Text Vectorization Layer** <br>\n",
    "more details [here](https://keras.io/api/layers/preprocessing_layers/core_preprocessing_layers/text_vectorization/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14dd646f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-28T13:52:06.718266Z",
     "start_time": "2022-03-28T13:52:06.693271Z"
    }
   },
   "outputs": [],
   "source": [
    "from keras.layers.experimental.preprocessing import TextVectorization\n",
    "import tensorflow as tf\n",
    "from keras.layers import LSTM\n",
    "\n",
    "\n",
    "text_dataset = tf.data.Dataset.from_tensor_slices(df[\"opinion_text\"])\n",
    "max_features = 10000  # Maximum vocab size.\n",
    "max_len = 2000  # Sequence length to pad the outputs to.\n",
    "\n",
    "# Create the layer.  \n",
    "vectorize_layer = TextVectorization(\n",
    " max_tokens=max_features,\n",
    " output_mode='int',\n",
    " output_sequence_length=max_len)\n",
    "# Now that the vocab layer has been created, call `adapt` on the text-only  \n",
    "# dataset to create the vocabulary. You don't have to batch, but for large  \n",
    "# datasets this means we're not keeping spare copies of the dataset.\n",
    "\n",
    "\n",
    "vectorize_layer.adapt(text_dataset.batch(64))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d311667d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-28T13:52:12.399103Z",
     "start_time": "2022-03-28T13:52:12.369886Z"
    }
   },
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential()\n",
    "\n",
    "\n",
    "\n",
    "model.add(tf.keras.Input(shape=(1,), dtype=tf.string))\n",
    "model.add(vectorize_layer)\n",
    "model.add(Embedding(max_features, 64, name=\"embedding_layer\"))\n",
    "model.add(LSTM(64))\n",
    "model.add(Dense(64, activation=\"relu\"))\n",
    "model.add(Dense(1, activation=\"sigmoid\")) # output layer\n",
    "model.summary()\n",
    "\n",
    "model.compile(loss='binary_crossentropy', # cost function\n",
    "              optimizer='adam', # use adam as the optimizer\n",
    "              metrics=['accuracy']) # compute accuracy, for scoring\n",
    "\n",
    "model_info = model.fit(df[\"opinion_text\"], Y, \n",
    "                      epochs=3,\n",
    "                      validation_split=.2, batch_size=32)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb059bc9",
   "metadata": {},
   "source": [
    "**Deep learning tips, tricks and advanced features**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd958785",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-28T13:52:24.881012Z",
     "start_time": "2022-03-28T13:52:24.573352Z"
    }
   },
   "outputs": [],
   "source": [
    "# Set up a basic model again for advanced features.\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Activation, Dense\n",
    "model = Sequential()\n",
    "# set custom activation, specify input dim\n",
    "model.add(Dense(64, input_dim=1000, activation='gelu')) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a630090",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-28T13:52:31.224246Z",
     "start_time": "2022-03-28T13:52:31.191604Z"
    }
   },
   "outputs": [],
   "source": [
    "# initializers\n",
    "model.add(Dense(64, kernel_initializer='he_normal'))\n",
    "model.add(Dense(64, kernel_initializer='he_uniform'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e2506ed",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-28T13:52:35.672196Z",
     "start_time": "2022-03-28T13:52:35.647331Z"
    }
   },
   "outputs": [],
   "source": [
    "# other activation functions (https://keras.io/activations/)\n",
    "model.add(Dense(64, activation=\"elu\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00715651",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-28T13:52:39.947691Z",
     "start_time": "2022-03-28T13:52:39.936613Z"
    }
   },
   "outputs": [],
   "source": [
    "# batch normalization\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "model.add(Dense(64, use_bias=False)) \n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "292e4f21",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-28T13:52:45.714411Z",
     "start_time": "2022-03-28T13:52:45.679221Z"
    }
   },
   "outputs": [],
   "source": [
    "# regularization\n",
    "from keras.regularizers import l1, l2, l1_l2\n",
    "model.add(Dense(64, \n",
    "                kernel_regularizer=l2(0.01),\n",
    "                activity_regularizer=l1(0.01)))\n",
    "model.add(Dense(64, \n",
    "                kernel_regularizer=l1_l2(l1=0.01,l2=.01),\n",
    "                activity_regularizer=l1_l2(l1=0.01,l2=.01)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e24206ff",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-28T13:52:50.267897Z",
     "start_time": "2022-03-28T13:52:50.254319Z"
    }
   },
   "outputs": [],
   "source": [
    "# Dropout\n",
    "from keras.layers import Dropout\n",
    "# np.random.rand(1000)\n",
    "model.add(Dropout(0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2efb5a20",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-28T13:52:54.944225Z",
     "start_time": "2022-03-28T13:52:54.839028Z"
    }
   },
   "outputs": [],
   "source": [
    "model.add(Dense(1,activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15f17b7f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-28T13:52:59.699546Z",
     "start_time": "2022-03-28T13:52:59.676350Z"
    }
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e56b9f5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-28T13:53:04.778410Z",
     "start_time": "2022-03-28T13:53:04.746811Z"
    }
   },
   "outputs": [],
   "source": [
    "# Optimizers\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.compile(optimizer='sgd',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# different loss functions\n",
    "\n",
    "model.compile(optimizer='sgd',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7fa2cc6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-28T13:53:09.279180Z",
     "start_time": "2022-03-28T13:53:09.256501Z"
    }
   },
   "outputs": [],
   "source": [
    "# Early stopping\n",
    "from keras.callbacks import EarlyStopping\n",
    "earlystop = EarlyStopping(monitor='val_accuracy', \n",
    "                          min_delta=0.0001, \n",
    "                          patience=5, \n",
    "                          mode='auto')\n",
    "\n",
    "\n",
    "model.fit(X.todense(), Y, batch_size=128, \n",
    "           epochs=100, \n",
    "           callbacks=[earlystop], \n",
    "           validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfc3a8e7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-28T13:53:14.483691Z",
     "start_time": "2022-03-28T13:53:14.415420Z"
    }
   },
   "outputs": [],
   "source": [
    "# Batch Training with Large Data\n",
    "from numpy import memmap\n",
    "X_mm = memmap('X.pkl',shape=(768, 1000))\n",
    "\n",
    "model.fit(X_mm, Y, batch_size=128, \n",
    "           epochs=3, \n",
    "           validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff3cdefe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grid search with KerasClassifier\n",
    "\n",
    "\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# instantiate KerasClassifier with build function\n",
    "def create_model(hidden_layers=1):  \n",
    "    model = Sequential()\n",
    "    model.add(Dense(16, input_dim=1000, \n",
    "                    activation='relu')) \n",
    "    for i in range(hidden_layers):\n",
    "        model.add(Dense(8, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(loss='binary_crossentropy', \n",
    "                optimizer='adam', \n",
    "                metrics= ['accuracy'])\n",
    "    return model\n",
    "\n",
    "clf = KerasClassifier(create_model)\n",
    "\n",
    "# set of grid search CV to select number of hidden layers\n",
    "params = {'hidden_layers' : [0,1,2,3]}\n",
    "grid = GridSearchCV(clf, param_grid=params)\n",
    "grid.fit(X.todense(),Y)\n",
    "grid.best_params_"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
