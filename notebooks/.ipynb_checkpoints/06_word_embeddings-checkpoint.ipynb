{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-22T21:27:00.669557Z",
     "start_time": "2022-03-22T21:27:00.538041Z"
    }
   },
   "outputs": [],
   "source": [
    "import warnings; warnings.simplefilter('ignore')\n",
    "# set this to your working directory\n",
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "df = pd.read_pickle('sc_cases_cleaned.pkl',compression='gzip')\n",
    "df=df.reset_index(drop=True)\n",
    "df = df.assign(author_id=(df['authorship']).astype('category').cat.codes)\n",
    "df = df[pd.notnull(df['authorship'])] # drop cases without an author\n",
    "import numpy as np\n",
    "vocab = pd.read_pickle('vocab.pkl')\n",
    "from IPython.display import SVG\n",
    "from keras.utils.vis_utils import model_to_dot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-22T21:27:01.696494Z",
     "start_time": "2022-03-22T21:27:01.682311Z"
    }
   },
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entity Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make judge dummy variables\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "encoder = LabelEncoder()\n",
    "J = encoder.fit_transform(df['author_id'].astype(str))\n",
    "num_judges = max(J)+1\n",
    "Y = df['x_republican'] > 0\n",
    "Y2 = df['log_cite_count']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "J[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up DNN\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, Flatten\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(num_judges, # number of categories\n",
    "                    2, # dimensions of embedding\n",
    "                    input_length=1)) \n",
    "model.add(Flatten()) # needed after Embedding\n",
    "model.add(Dense(2))\n",
    "model.add(Dense(1,activation='sigmoid'))\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "#dot = model_to_dot(model,\n",
    "#                   show_shapes=True,\n",
    "#                   show_layer_names=False)\n",
    "#SVG(dot.create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the Judge Vectors\n",
    "#!pip install ggplot\n",
    "import seaborn as sns\n",
    "sns.set_theme()\n",
    "\n",
    "judge_cites = dict(Y.groupby(J).mean())\n",
    "df2 = pd.DataFrame(J,columns=['authorship']).drop_duplicates().sort_values('authorship')\n",
    "df2['republican'] = df2['authorship'].apply(lambda x: judge_cites[x])\n",
    "\n",
    "for i in range(5):\n",
    "    if i > 0:\n",
    "        model.fit(J,Y,epochs=1, validation_split=.2)\n",
    "    \n",
    "    judge_vectors = model.layers[0].get_weights()[0]\n",
    "    df2['x'] = judge_vectors[:,0]\n",
    "    df2['y'] = judge_vectors[:,1]    \n",
    "\n",
    "    sns.relplot( data=df2, x=\"x\", y=\"y\", hue='republican', kind='scatter')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert documents to sequences of word indexes\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "num_words = 200\n",
    "tokenizer = Tokenizer(num_words=num_words)\n",
    "tokenizer.fit_on_texts(df['opinion_text'])\n",
    "sequences = tokenizer.texts_to_sequences(df['opinion_text'])\n",
    "len(sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# represent data as numrows x maxlen matrix\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "maxlen = max([len(sent) for sent in sequences]) \n",
    "maxlen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pad_sequences(sequences, maxlen=maxlen)\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X[0][maxlen-len(sequences[0]):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model setup\n",
    "model = Sequential()\n",
    "model.add(Embedding(num_words,\n",
    "                    2,\n",
    "                    input_length=maxlen)) # sequence length\n",
    "model.add(Flatten()) # 86*2 = 172 dims\n",
    "model.add(Dense(4))\n",
    "model.add(Dense(1))\n",
    "model.compile(optimizer='adam',loss='binary_crossentropy')\n",
    "from IPython.display import HTML\n",
    "style = \"<style>svg{width:50% !important;height:50% !important;}</style>\"\n",
    "HTML(style)\n",
    "dot = model_to_dot(model, show_shapes=True, show_layer_names=False,  dpi=70)\n",
    "SVG(dot.create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show the vectors\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "df3 = pd.DataFrame(list(tokenizer.word_index.items()),\n",
    "                  columns=['word', 'word_index']).sort_values('word_index')[:num_words]\n",
    "\n",
    "for i in range(10):\n",
    "    if i > 0:\n",
    "        model.fit(X,Y,epochs=1, validation_split=.2)\n",
    "\n",
    "    word_vectors = model.layers[0].get_weights()[0]\n",
    "    df3['x'] = word_vectors[:,0]\n",
    "    df3['y'] = word_vectors[:,1]\n",
    "\n",
    "    plot = sns.relplot( data=df3, x=\"x\", y=\"y\", s=0, height=8.27, aspect=11.7/8.27)\n",
    "\n",
    "    for row in df3.itertuples():\n",
    "        ax = plot.axes[0, 0]\n",
    "        ax.text(row.x, row.y, row.word, horizontalalignment='left')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word Similarity\n",
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "vec_defendants = word_vectors[tokenizer.word_index['defendant']-1]\n",
    "vec_sent = word_vectors[tokenizer.word_index['sentence']-1]\n",
    "vec_against = word_vectors[tokenizer.word_index['against']-1]\n",
    "\n",
    "print(1-cosine(vec_defendants, vec_convicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(1-cosine(vec_defendants, vec_against))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec in gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word2vec requires sentences as input\n",
    "from nltk import sent_tokenize\n",
    "from string import punctuation\n",
    "translator = str.maketrans('','',punctuation) \n",
    "from nltk.corpus import stopwords\n",
    "stoplist = set(stopwords.words('english'))\n",
    "from nltk.stem import SnowballStemmer\n",
    "stemmer = SnowballStemmer('english')\n",
    "\n",
    "def normalize_text(doc):\n",
    "    \"Input doc and return clean list of tokens\"\n",
    "    doc = doc.replace('\\r', ' ').replace('\\n', ' ')\n",
    "    lower = doc.lower() # all lower case\n",
    "    nopunc = lower.translate(translator) # remove punctuation\n",
    "    words = nopunc.split() # split into tokens\n",
    "    nostop = [w for w in words if w not in stoplist] # remove stopwords\n",
    "    no_numbers = [w if not w.isdigit() else '#' for w in nostop] # normalize numbers\n",
    "    stemmed = [stemmer.stem(w) for w in no_numbers] # stem each word\n",
    "    return stemmed\n",
    "\n",
    "def get_sentences(doc):\n",
    "    sent=[]\n",
    "    for raw in sent_tokenize(doc):\n",
    "        raw2 = normalize_text(raw)\n",
    "        sent.append(raw2)\n",
    "    return sent\n",
    "\n",
    "sentences = []\n",
    "for doc in df['opinion_text']:\n",
    "    sentences += get_sentences(doc)\n",
    "from random import shuffle\n",
    "shuffle(sentences) # stream in sentences in random order\n",
    "\n",
    "# train the model\n",
    "from gensim.models import Word2Vec\n",
    "w2v = Word2Vec(sentences,  # list of tokenized sentences\n",
    "               workers = 8, # Number of threads to run in parallel\n",
    "               size=300,  # Word vector dimensionality     \n",
    "               min_count =  25, # Minimum word count  \n",
    "               window = 5, # Context window size      \n",
    "               sample = 1e-3, # Downsample setting for frequent words\n",
    "               )\n",
    "\n",
    "# done training, so delete context vectors\n",
    "w2v.init_sims(replace=True)\n",
    "\n",
    "w2v.save('w2v-vectors.pkl')\n",
    "\n",
    "w2v.wv['judg'] # vector for \"judge\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v.wv.similarity('judg','juri') # similarity "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v.wv.most_similar('judg') # most similar words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word2vec requires sentences as input\n",
    "from nltk import sent_tokenize\n",
    "from string import punctuation\n",
    "translator = str.maketrans('','',punctuation) \n",
    "from nltk.corpus import stopwords\n",
    "stoplist = set(stopwords.words('english'))\n",
    "from nltk.stem import SnowballStemmer\n",
    "stemmer = SnowballStemmer('english')\n",
    "\n",
    "def normalize_text(doc):\n",
    "    \"Input doc and return clean list of tokens\"\n",
    "    doc = doc.replace('\\r', ' ').replace('\\n', ' ')\n",
    "    lower = doc.lower() # all lower case\n",
    "    nopunc = lower#.translate(translator) # remove punctuation\n",
    "    words = nopunc.split() # split into tokens\n",
    "    nostop = [w for w in words ]#if w not in stoplist] # remove stopwords\n",
    "    no_numbers = [w if not w.isdigit() else '#' for w in nostop] # normalize numbers\n",
    "    stemmed = [stemmer.stem(w) for w in no_numbers] # stem each word\n",
    "    return stemmed\n",
    "\n",
    "def get_sentences(doc):\n",
    "    sent=[]\n",
    "    for raw in sent_tokenize(doc):\n",
    "        raw2 = normalize_text(raw)\n",
    "        sent.append(raw2)\n",
    "    return sent\n",
    "\n",
    "sentences = []\n",
    "for doc in df['opinion_text']:\n",
    "    sentences += get_sentences(doc)\n",
    "from random import shuffle\n",
    "shuffle(sentences) # stream in sentences in random order\n",
    "\n",
    "# train the model\n",
    "from gensim.models import Word2Vec\n",
    "w2v = Word2Vec(sentences,  # list of tokenized sentences\n",
    "               workers = 8, # Number of threads to run in parallel\n",
    "               size=300,  # Word vector dimensionality     \n",
    "               min_count =  25, # Minimum word count  \n",
    "               window = 5, # Context window size      \n",
    "               sample = 1e-3, # Downsample setting for frequent words\n",
    "               )\n",
    "\n",
    "# done training, so delete context vectors\n",
    "w2v.init_sims(replace=True)\n",
    "\n",
    "w2v.save('w2v-vectors.pkl')\n",
    "\n",
    "w2v.wv['judg'] # vector for \"judge\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v.wv.similarity('judg','juri') # similarity "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v.wv.most_similar('judg') # most similar words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# analogies: judge is to man as __ is to woman\n",
    "w2v.wv.most_similar(positive=['judg','man'],\n",
    "                 negative=['woman'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word2Vec: K-Means Clusters\n",
    "from sklearn.cluster import KMeans\n",
    "kmw = KMeans(n_clusters=50)\n",
    "kmw.fit(w2v.wv.vectors)\n",
    "judge_clust = kmw.labels_[w2v.wv.vocab['judg'].index]\n",
    "for i, cluster in enumerate(kmw.labels_):\n",
    "    if cluster == judge_clust and i<=100:\n",
    "        print(w2v.wv.index2word[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-trained vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "en = spacy.load('en_core_web_sm')\n",
    "apple = en('apple') \n",
    "apple.vector # vector for 'apple'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apple.similarity(apple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orange = en('orange')\n",
    "apple.similarity(orange)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "#!python -m spacy download de_core_news_sm\n",
    "de = spacy.load('de_core_news_sm')\n",
    "apfel = de('apfel')\n",
    "orange = de('orange')\n",
    "apfel.similarity(orange)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing an embedding layer with pre-trained vectors\n",
    "embed_dims = len(apple.vector)\n",
    "embedding_matrix = np.zeros([num_words, embed_dims])\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    if i > num_words:\n",
    "        break\n",
    "    embedding_vector = en(word).vector\n",
    "    embedding_matrix[i-1] = embedding_vector    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(num_words,\n",
    "                    embed_dims,\n",
    "                    weights=[embedding_matrix],\n",
    "                    input_length=maxlen,\n",
    "                    trainable=False)) # frozen layer\n",
    "model.add(Flatten()) # 86*300 = 25800 dims\n",
    "model.add(Dense(64,activation='relu'))\n",
    "model.add(Dense(1))\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show the vectors\n",
    "from sklearn.manifold import TSNE\n",
    "tsne = TSNE(n_components=2, verbose=1, perplexity=50, n_iter=300)\n",
    "\n",
    "df3 = pd.DataFrame(list(tokenizer.word_index.items()),\n",
    "                  columns=['word', 'word_index']).sort_values('word_index')[:num_words]\n",
    "\n",
    "for i in range(3):\n",
    "    if i > 0:\n",
    "        model.fit(X,Y,epochs=1, validation_split=.2)\n",
    "    \n",
    "    word_vectors = model.layers[0].get_weights()[0]\n",
    "    wv_tsne = tsne.fit_transform(word_vectors)\n",
    "\n",
    "    df3['x'] = wv_tsne[:,0]\n",
    "    df3['y'] = wv_tsne[:,1]\n",
    "    plot = sns.relplot( data=df3, x=\"x\", y=\"y\", s=0, height=8.27, aspect=11.7/8.27)\n",
    "\n",
    "    for row in df3.itertuples():\n",
    "        ax = plot.axes[0, 0]\n",
    "        ax.text(row.x, row.y, row.word, horizontalalignment='left')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GloVe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install glove-python-binary for python >3.6\n",
    "#!pip install glove_python for other versions\n",
    "\n",
    "import itertools\n",
    "from glove import Corpus, Glove\n",
    "\n",
    "from nltk import sent_tokenize\n",
    "from string import punctuation\n",
    "translator = str.maketrans('','',punctuation) \n",
    "from nltk.corpus import stopwords\n",
    "stoplist = set(stopwords.words('english'))\n",
    "from nltk.stem import SnowballStemmer\n",
    "stemmer = SnowballStemmer('english')\n",
    "\n",
    "def normalize_text(doc):\n",
    "    \"Input doc and return clean list of tokens\"\n",
    "    doc = doc.replace('\\r', ' ').replace('\\n', ' ')\n",
    "    lower = doc.lower() # all lower case\n",
    "    nopunc = lower.translate(translator) # remove punctuation\n",
    "    words = nopunc.split() # split into tokens\n",
    "    nostop = [w for w in words if w not in stoplist] # remove stopwords\n",
    "    no_numbers = [w if not w.isdigit() else '#' for w in nostop] # normalize numbers\n",
    "    stemmed = [stemmer.stem(w) for w in no_numbers] # stem each word\n",
    "    return stemmed\n",
    "\n",
    "def get_sentences(doc):\n",
    "    sent=[]\n",
    "    for raw in sent_tokenize(doc):\n",
    "        raw2 = normalize_text(raw)\n",
    "        sent.append(raw2)\n",
    "    return sent\n",
    "\n",
    "sentences = []\n",
    "for doc in df['opinion_text']:\n",
    "    sentences += get_sentences(doc)\n",
    "from random import shuffle\n",
    "shuffle(sentences) # stream in sentences in random order\n",
    "\n",
    "corpus = Corpus()\n",
    "corpus.fit(sentences, window=10)\n",
    "glove = Glove(no_components=100, learning_rate=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove.fit(corpus.matrix, epochs=30, no_threads=4, verbose=True)\n",
    "glove.add_dictionary(corpus.dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove.word_vectors[glove.dictionary['judg']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove.most_similar('judg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Word Mover Distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import wmd\n",
    "nlp = spacy.load('en_core_web_sm', \n",
    "                 create_pipeline=wmd.WMD.create_spacy_pipeline)\n",
    "doc1 = nlp(\"Politician speaks to the media in Illinois.\")\n",
    "doc2 = nlp(\"The president greets the press in Chicago.\")\n",
    "print(doc1.similarity(doc2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embedding Lookup\n",
    "\n",
    "Keras provides functionality to feed just words (actually indices of words) as model input. The model then performs an embedding lookup (we go from sparse one-hot to dense) which then becomes the input for further computation in the model. For a more detailed tutorial, have a look [here](https://machinelearningmastery.com/use-word-embedding-layers-deep-learning-keras/). \n",
    "\n",
    "First, we have to pre-process the data once again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-22T21:27:10.491277Z",
     "start_time": "2022-03-22T21:27:09.065853Z"
    }
   },
   "outputs": [],
   "source": [
    "#df['opinion_text']\n",
    "\n",
    "from keras.preprocessing.text import text_to_word_sequence\n",
    "\n",
    "# tokenize the text\n",
    "\n",
    "\n",
    "tokenized = [text_to_word_sequence(opinion) for opinion in df[\"opinion_text\"]]\n",
    "print (tokenized[0][:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-22T21:27:12.703843Z",
     "start_time": "2022-03-22T21:27:12.385188Z"
    }
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "counter = Counter()\n",
    "for i in tokenized:\n",
    "        counter.update(i)\n",
    "print (counter.most_common(10))\n",
    "num_words = len(counter)\n",
    "print (num_words) ## 58'787\n",
    "print (max(len(i) for i in tokenized)) # 26'097, this is one of the challenges of working with legal text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-22T21:27:43.202043Z",
     "start_time": "2022-03-22T21:27:41.227554Z"
    }
   },
   "outputs": [],
   "source": [
    "# create one_hot representation for each word\n",
    "from keras.preprocessing.text import one_hot\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "length_vocab = 10000\n",
    "X_one_hot = [one_hot(opinion, n=length_vocab) for opinion in df[\"opinion_text\"]]\n",
    "print (X_one_hot[0][:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-22T21:27:48.551254Z",
     "start_time": "2022-03-22T21:27:48.483160Z"
    }
   },
   "outputs": [],
   "source": [
    "# next, we pad (or truncate) such that all the inputs have same length\n",
    "\n",
    "max_seq_length = 2000\n",
    "X_one_hot_padded = pad_sequences(X_one_hot, padding='post', maxlen=max_seq_length, truncating='post')\n",
    "X_one_hot_padded.shape # (768, 2000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Embedding lookup**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-22T21:28:52.541686Z",
     "start_time": "2022-03-22T21:28:52.514639Z"
    }
   },
   "outputs": [],
   "source": [
    "from keras.layers import Embedding\n",
    "from keras.models import Sequential\n",
    "\n",
    "model = Sequential() # create a sequential model\n",
    "model.add(Embedding(length_vocab, 64, input_length=max_seq_length, name=\"embedding_layer\"))\n",
    "model.summary() #640'000 params because 64 dim for 10'000 words\n",
    "\n",
    "# that's it\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
