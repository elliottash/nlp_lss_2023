{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Document Embeddings**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-08T18:19:28.297933Z",
     "start_time": "2022-04-08T18:13:14.779550Z"
    }
   },
   "outputs": [],
   "source": [
    "###\n",
    "# Setup\n",
    "###\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "df1 = pd.read_csv('death-penalty-cases.csv')\n",
    "\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# word2vec requires sentences as input\n",
    "sentences = []\n",
    "for doc in df1['snippet']:\n",
    "    sents = [sent for sent in nlp(doc).sents]\n",
    "    sentences += sents\n",
    "#     sentences += get_sentences(doc)\n",
    "from random import shuffle\n",
    "shuffle(sentences) # stream in sentences in random order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-10T10:38:32.760156Z",
     "start_time": "2022-04-10T10:38:32.745346Z"
    }
   },
   "outputs": [],
   "source": [
    "[w. for w in sentences[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Average word2vec Embeddings**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-10T10:39:51.710719Z",
     "start_time": "2022-04-10T10:39:48.834360Z"
    }
   },
   "outputs": [],
   "source": [
    "###\n",
    "# Make document vectors by averaging word embeddings in a document\n",
    "##\n",
    "\n",
    "# Continuous bag-of-words representation\n",
    "from gensim.models import Word2Vec\n",
    "w2v = Word2Vec.load('w2v-vectors.pkl')\n",
    "\n",
    "sentvecs = []\n",
    "for sentence in sentences:\n",
    "    vecs = [w2v.wv[w.text] for w in sentence if w.text in w2v.wv]\n",
    "    if len(vecs)== 0:\n",
    "        sentvecs.append(np.nan)\n",
    "        continue\n",
    "    sentvec = np.mean(vecs,axis=0)\n",
    "    sentvecs.append(sentvec.reshape(1,-1))\n",
    "sentvecs[0][0][:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-10T10:40:16.837807Z",
     "start_time": "2022-04-10T10:40:16.821731Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "cosine_similarity(sentvecs[0],\n",
    "                  sentvecs[1])[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Doc2Vec**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-10T10:40:27.456734Z",
     "start_time": "2022-04-10T10:40:20.500410Z"
    }
   },
   "outputs": [],
   "source": [
    "###\n",
    "# Doc2Vec\n",
    "###\n",
    "\n",
    "from nltk import word_tokenize\n",
    "docs = []\n",
    "\n",
    "for i, row in df1.iterrows():\n",
    "    docs += [word_tokenize(row['snippet'])]\n",
    "shuffle(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-10T10:41:09.612012Z",
     "start_time": "2022-04-10T10:41:02.740837Z"
    }
   },
   "outputs": [],
   "source": [
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "doc_iterator = [TaggedDocument(doc, [i]) for i, doc in enumerate(docs)]\n",
    "d2v = Doc2Vec(doc_iterator,\n",
    "                min_count=10, # minimum word count\n",
    "                window=10,    # window size\n",
    "                vector_size=100, # size of document vector\n",
    "                sample=1e-4, \n",
    "                negative=5, \n",
    "                workers=4, # threads\n",
    "                #dbow_words = 1 # uncomment to get word vectors too\n",
    "                max_vocab_size=1000) # max vocab size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-10T10:41:09.679323Z",
     "start_time": "2022-04-10T10:41:09.612958Z"
    }
   },
   "outputs": [],
   "source": [
    "d2v.save('d2v-vectors.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-10T10:41:09.682503Z",
     "start_time": "2022-04-10T10:41:09.680192Z"
    }
   },
   "outputs": [],
   "source": [
    "# matrix of all document vectors:\n",
    "D = d2v.docvecs.vectors_docs\n",
    "D.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-10T10:41:09.686064Z",
     "start_time": "2022-04-10T10:41:09.683736Z"
    }
   },
   "outputs": [],
   "source": [
    "D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-10T10:41:09.689379Z",
     "start_time": "2022-04-10T10:41:09.686824Z"
    }
   },
   "outputs": [],
   "source": [
    "# infer vectors for new documents\n",
    "a = d2v.infer_vector(['the judge on the court'])\n",
    "\n",
    "b = d2v.infer_vector(['the jury and the judge'])\n",
    "\n",
    "c = d2v.infer_vector(['cats hunt mice'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-10T10:41:12.238851Z",
     "start_time": "2022-04-10T10:41:12.221000Z"
    }
   },
   "outputs": [],
   "source": [
    "a.shape, b.shape\n",
    "print(cosine_similarity(np.expand_dims(a, axis=0), np.expand_dims(b, axis=0)))\n",
    "print(cosine_similarity(np.expand_dims(a, axis=0), np.expand_dims(c, axis=0)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-10T10:41:15.479461Z",
     "start_time": "2022-04-10T10:41:13.132483Z"
    }
   },
   "outputs": [],
   "source": [
    "# get all pair-wise document similarities\n",
    "pairwise_sims = cosine_similarity(D)\n",
    "pairwise_sims.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-10T10:41:15.483026Z",
     "start_time": "2022-04-10T10:41:15.480593Z"
    }
   },
   "outputs": [],
   "source": [
    "pairwise_sims[:3,:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-10T10:41:26.808072Z",
     "start_time": "2022-04-10T10:41:16.434202Z"
    }
   },
   "outputs": [],
   "source": [
    "# Document clusters\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# create 50 clusters of similar documents\n",
    "num_clusters = 50\n",
    "kmw = KMeans(n_clusters=num_clusters)\n",
    "kmw.fit(D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-10T10:41:26.814680Z",
     "start_time": "2022-04-10T10:41:26.809182Z"
    }
   },
   "outputs": [],
   "source": [
    "# Documents from an example cluster\n",
    "for i, doc in enumerate(docs):\n",
    "    if kmw.labels_[i] == 25:\n",
    "        print(' '.join(doc[:9]))\n",
    "    if i == 1000:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-10T10:41:35.580953Z",
     "start_time": "2022-04-10T10:41:35.572012Z"
    }
   },
   "outputs": [],
   "source": [
    "# t-SNE for visualization\n",
    "# from sklearn.manifold import TSNE\n",
    "# tsne = TSNE(n_components=2, verbose=1, perplexity=50, n_iter=300)\n",
    "# d2v_tsne = tsne.fit_transform(D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-10T10:41:35.985482Z",
     "start_time": "2022-04-10T10:41:35.790326Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=3,svd_solver='randomized')\n",
    "Xpca = pca.fit_transform(D)\n",
    "pca.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-10T10:41:39.170694Z",
     "start_time": "2022-04-10T10:41:39.165484Z"
    }
   },
   "outputs": [],
   "source": [
    "#vdf = pd.DataFrame(Xpca,\n",
    "#                  columns=['x-tsne', 'y-tsne'])\n",
    "#vdf['cluster'] = kmw.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-10T10:41:39.949600Z",
     "start_time": "2022-04-10T10:41:39.338877Z"
    }
   },
   "outputs": [],
   "source": [
    "#%% PCA Viz\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#plt.scatter(Xpca[:,0],Xpca[:,1], alpha=.1)\n",
    "\n",
    "cdict = {1: 'red', 2: 'blue', 3: 'green'}\n",
    "fig, ax = plt.subplots()\n",
    "#for g, label in cdict.items():\n",
    "for g in np.unique(kmw.labels_):\n",
    "    ix = np.where(kmw.labels_ == g)\n",
    "    #ix = np.where(kmw == g)\n",
    "    #    ax.scatter(scatter_x[ix], scatter_y[ix], c = cdict[g], label = g, s = 100)\n",
    "    if g in cdict:\n",
    "        # use color from cdict\n",
    "        color = cdict[g]\n",
    "        ax.scatter(Xpca[:,0][ix], Xpca[:,1][ix], c = color, label = g, s = 100, alpha=0.1)\n",
    "    else:\n",
    "        if g < 10:\n",
    "            color = \"black\"\n",
    "            ax.scatter(Xpca[:,0][ix], Xpca[:,1][ix], c = color, label = g, s = 100, alpha=0.01)\n",
    "    \n",
    "\n",
    "        \n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-10T10:41:43.032345Z",
     "start_time": "2022-04-10T10:41:43.021170Z"
    }
   },
   "outputs": [],
   "source": [
    "# sample 100 texts to show different document embeddings techniques\n",
    "\n",
    "texts = df1[\"snippet\"][:100]\n",
    "text = texts[0]\n",
    "text, texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Other types of Document Embeddings**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-10T10:41:45.050646Z",
     "start_time": "2022-04-10T10:41:44.691202Z"
    }
   },
   "outputs": [],
   "source": [
    "# spacy embeddings (= averaged gloVe embeddings)\n",
    "\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "doc = nlp(text)\n",
    "doc.vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Uinversal Sentence Encoder**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-10T10:41:48.718613Z",
     "start_time": "2022-04-10T10:41:46.974338Z"
    }
   },
   "outputs": [],
   "source": [
    "# universal sentence encoder\n",
    "!pip install --upgrade tensorflow-hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-10T10:42:12.384011Z",
     "start_time": "2022-04-10T10:41:50.383876Z"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_eager_execution()\n",
    "\n",
    "import tensorflow_hub as hub\n",
    "import numpy as np\n",
    "\n",
    "module_url = \"https://tfhub.dev/google/universal-sentence-encoder/4\"\n",
    "model = hub.load(module_url)\n",
    "print (\"module %s loaded\" % module_url)\n",
    "def embed(input):\n",
    "    return model(input)\n",
    "\n",
    "embeddings = embed([text])\n",
    "\n",
    "with tf.Session() as session:\n",
    "    session.run([tf.global_variables_initializer(), tf.tables_initializer()])\n",
    "    text_encoded = session.run(embeddings)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-10T10:42:12.389495Z",
     "start_time": "2022-04-10T10:42:12.384981Z"
    }
   },
   "outputs": [],
   "source": [
    "text_encoded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SBERT**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-10T10:42:13.839087Z",
     "start_time": "2022-04-10T10:42:12.390263Z"
    }
   },
   "outputs": [],
   "source": [
    "!pip install sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-10T10:42:39.299350Z",
     "start_time": "2022-04-10T10:42:13.842068Z"
    }
   },
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "model = \"bert-base-nli-mean-tokens\"\n",
    "embedder = SentenceTransformer(model)\n",
    "text_encoded = embedder.encode([text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-10T10:42:39.306496Z",
     "start_time": "2022-04-10T10:42:39.300454Z"
    }
   },
   "outputs": [],
   "source": [
    "text_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
