{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dbe9d25b",
   "metadata": {
    "id": "dbe9d25b"
   },
   "source": [
    "# HW05: Word Embeddings\n",
    "\n",
    "Remember that these homework work as a completion grade. **You can <span style=\"color:red\">not</span> skip one section this homework.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b928af6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8b3a0596",
   "metadata": {
    "id": "8b3a0596"
   },
   "source": [
    "**Essay Feedback**\n",
    "\n",
    "Please provide feedback to two classmates' essays on Eduflow."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ea14794",
   "metadata": {
    "id": "5ea14794"
   },
   "source": [
    "**Training word2vec**\n",
    "\n",
    "In this section, we train a word2vec model using gensim. We train the model on text8 (which consists of the first 90M characters of a Wikipedia dump from 2006 and is considered one of the benchmarks for evaluating language models)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "95a38d6e",
   "metadata": {
    "id": "95a38d6e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'num_records': 1701,\n",
       " 'record_format': 'list of str (tokens)',\n",
       " 'file_size': 33182058,\n",
       " 'reader_code': 'https://github.com/RaRe-Technologies/gensim-data/releases/download/text8/__init__.py',\n",
       " 'license': 'not found',\n",
       " 'description': 'First 100,000,000 bytes of plain text from Wikipedia. Used for testing purposes; see wiki-english-* for proper full Wikipedia datasets.',\n",
       " 'checksum': '68799af40b6bda07dfa47a32612e5364',\n",
       " 'file_name': 'text8.gz',\n",
       " 'read_more': ['http://mattmahoney.net/dc/textdata.html'],\n",
       " 'parts': 1}"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gensim.downloader as api\n",
    "\n",
    "api.info(\"text8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "0a49444c",
   "metadata": {
    "id": "0a49444c"
   },
   "outputs": [],
   "source": [
    "dataset = api.load(\"text8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "2f9c2a32",
   "metadata": {},
   "outputs": [],
   "source": [
    "#data = [d for d in dataset]\n",
    "#data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "61fa38b5",
   "metadata": {
    "id": "61fa38b5"
   },
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "##TODO train a word2vec model on this dataset which appear at least 10 times in the corpus\n",
    "model = Word2Vec(dataset, min_count=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4af69360",
   "metadata": {
    "id": "4af69360"
   },
   "source": [
    "**Word Similarities**\n",
    "\n",
    "gensim models provide almost all the utility you might want to wish for to perform standard word similarity tasks. They are available in the .wv (wordvectors) attribute of the model, more details could be found [here](https://radimrehurek.com/gensim/models/keyedvectors.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "5cf99280",
   "metadata": {
    "id": "5cf99280"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('prince', 0.756159245967865),\n",
       " ('queen', 0.7254481315612793),\n",
       " ('kings', 0.6992501020431519),\n",
       " ('emperor', 0.6969693303108215),\n",
       " ('regent', 0.6830177307128906),\n",
       " ('vii', 0.6797141432762146),\n",
       " ('constantine', 0.6772857904434204),\n",
       " ('throne', 0.6630407571792603),\n",
       " ('pope', 0.6626625061035156),\n",
       " ('viii', 0.6589317917823792)]"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_vectors = model.wv\n",
    "\n",
    "##TODO find the closest words to king\n",
    "word_vectors.most_similar(positive='king')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "e808c1a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('bishops', 0.5292506217956543),\n",
       " ('nobles', 0.5209300518035889),\n",
       " ('monarchs', 0.5037956833839417),\n",
       " ('kings', 0.4996193051338196),\n",
       " ('scots', 0.49918463826179504),\n",
       " ('catholics', 0.4922649562358856),\n",
       " ('popes', 0.48591870069503784),\n",
       " ('judges', 0.48078009486198425),\n",
       " ('priests', 0.47934818267822266),\n",
       " ('clergy', 0.47883933782577515)]"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##TODO find the closest word for the vector \"woman\" + \"king\" - \"man\"\n",
    "\n",
    "word_vectors.most_similar(positive=['king', 'women'], negative='man')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c30c847",
   "metadata": {
    "id": "9c30c847"
   },
   "source": [
    "King is to man as woman is to X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2af37627",
   "metadata": {
    "id": "2af37627"
   },
   "source": [
    "**Evaluate Word Similarities** \n",
    "\n",
    "One common way to evaluate word2vec models are word analogy tasks. Let's check how good our model is on one of those. We consider the [WordSim353](http://alfonseca.org/eng/research/wordsim353.html) benchmark, the task is to determine how similar two words are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "71515b20",
   "metadata": {
    "id": "71515b20"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2023-03-30 17:02:13--  http://alfonseca.org/pubs/ws353simrel.tar.gz\n",
      "Resolving alfonseca.org (alfonseca.org)... 162.215.249.67\n",
      "Connecting to alfonseca.org (alfonseca.org)|162.215.249.67|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 5460 (5.3K) [application/x-gzip]\n",
      "Saving to: 'ws353simrel.tar.gz.4'\n",
      "\n",
      "ws353simrel.tar.gz. 100%[===================>]   5.33K  13.5KB/s    in 0.4s    \n",
      "\n",
      "2023-03-30 17:02:15 (13.5 KB/s) - 'ws353simrel.tar.gz.4' saved [5460/5460]\n",
      "\n",
      "[('tiger', 'cat'), ('tiger', 'tiger'), ('plane', 'car')] [7.35, 10.0, 5.77]\n"
     ]
    }
   ],
   "source": [
    "!wget http://alfonseca.org/pubs/ws353simrel.tar.gz\n",
    "!tar xf ws353simrel.tar.gz\n",
    "\n",
    "path = \"wordsim353_sim_rel/wordsim_similarity_goldstandard.txt\"\n",
    "\n",
    "def load_data(path):\n",
    "    X, y = [], []\n",
    "    with open(path) as f:\n",
    "        for line in f:\n",
    "            line = line.strip().split(\"\\t\")\n",
    "            X.append((line[0], line[1])) # each entry in x contains two words, e.g. X[0] = (tiger, cat)\n",
    "            y.append(float(line[-1])) # each entry in y is the annotation how similar two words are, e.g. Y[0] = 7.35\n",
    "    return X, y\n",
    "\n",
    "X, y = load_data(path)\n",
    "print (X[:3], y[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "9c8ced33",
   "metadata": {
    "id": "9c8ced33"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Arafat', 'Jackson')\n",
      "('asylum', 'madhouse')\n",
      "('cup', 'tableware')\n",
      "('Japanese', 'American')\n",
      "('Harvard', 'Yale')\n",
      "('Mexico', 'Brazil')\n",
      "('Mars', 'water')\n",
      "('Wednesday', 'news')\n",
      "('stock', 'CD')\n",
      "203\n"
     ]
    }
   ],
   "source": [
    "##TODO compute how similar the pairs in the WordSim353 are according to our model\n",
    "# if a word is not present in our model, we assign similarity 0 for the respective text pair\n",
    "\n",
    "similarity_scores = []\n",
    "\n",
    "for x in X:\n",
    "    try:\n",
    "        similarity_scores.append(word_vectors.similarity(x[0],x[1]))\n",
    "    except:\n",
    "        similarity_scores.append(0)\n",
    "        print(x)\n",
    "print(len(similarity_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "ebd47f93",
   "metadata": {
    "id": "ebd47f93"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SpearmanrResult(correlation=0.6535176250837023, pvalue=4.166416672262106e-26)"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.stats import spearmanr\n",
    "\n",
    "##TODO compute spearman's rank correlation between our prediction and the human annotations\n",
    "spearmanr(y, similarity_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "9ec86899",
   "metadata": {
    "id": "9ec86899"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SpearmanrResult(correlation=0.05420386995172911, pvalue=0.44243837493459515)"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "##TODO compute word similarities in the WordSim353 dataset using spaCy word embeddings\n",
    "\n",
    "similarity_list_2 = []\n",
    "\n",
    "for x in X:\n",
    "    d = nlp(x[0])\n",
    "    similarity_list_2.append(d.similarity(nlp(x[1])))\n",
    "\n",
    "##TODO compute spearman's rank correlation between these similarities and the human annotations\n",
    "# Don't worry if results are not too convincing for this experiment\n",
    "spearmanr(similarity_list_2, similarity_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d29de774",
   "metadata": {
    "id": "d29de774"
   },
   "source": [
    "**PyTorch Embeddings**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "3927e048",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-22T21:42:21.281177Z",
     "start_time": "2022-03-22T21:42:21.208787Z"
    },
    "id": "3927e048"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2023-03-30 17:02:19--  https://raw.githubusercontent.com/mhjabreel/CharCnn_Keras/master/data/ag_news_csv/train.csv\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.111.133, 185.199.108.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 29470338 (28M) [text/plain]\n",
      "Saving to: 'train.csv'\n",
      "\n",
      "train.csv           100%[===================>]  28.10M  2.42MB/s    in 13s     \n",
      "\n",
      "2023-03-30 17:02:32 (2.22 MB/s) - 'train.csv' saved [29470338/29470338]\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>title</th>\n",
       "      <th>lead</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>117622</th>\n",
       "      <td>business</td>\n",
       "      <td>Tension, fear after mall shooting</td>\n",
       "      <td>CAMBRIDGE -- In a quiet entryway of the Cambri...</td>\n",
       "      <td>Tension, fear after mall shooting CAMBRIDGE --...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49114</th>\n",
       "      <td>sci/tech</td>\n",
       "      <td>CNN Blog</td>\n",
       "      <td>Check this web log throughout the day as CNN A...</td>\n",
       "      <td>CNN Blog Check this web log throughout the day...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43708</th>\n",
       "      <td>world</td>\n",
       "      <td>Typhoon Meari Hits South Japan, Triggers Floods</td>\n",
       "      <td>TOKYO (Reuters) - A record eighth typhoon swe...</td>\n",
       "      <td>Typhoon Meari Hits South Japan, Triggers Flood...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112859</th>\n",
       "      <td>sport</td>\n",
       "      <td>Three teams in the running to be winter champi...</td>\n",
       "      <td>BERLIN: Three teams have the chance to lift th...</td>\n",
       "      <td>Three teams in the running to be winter champi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116500</th>\n",
       "      <td>sci/tech</td>\n",
       "      <td>Samsung, Sony in cross-license deal</td>\n",
       "      <td>Two of the world #39;s consumer electronics gi...</td>\n",
       "      <td>Samsung, Sony in cross-license deal Two of the...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           label                                              title  \\\n",
       "117622  business                  Tension, fear after mall shooting   \n",
       "49114   sci/tech                                           CNN Blog   \n",
       "43708      world    Typhoon Meari Hits South Japan, Triggers Floods   \n",
       "112859     sport  Three teams in the running to be winter champi...   \n",
       "116500  sci/tech                Samsung, Sony in cross-license deal   \n",
       "\n",
       "                                                     lead  \\\n",
       "117622  CAMBRIDGE -- In a quiet entryway of the Cambri...   \n",
       "49114   Check this web log throughout the day as CNN A...   \n",
       "43708    TOKYO (Reuters) - A record eighth typhoon swe...   \n",
       "112859  BERLIN: Three teams have the chance to lift th...   \n",
       "116500  Two of the world #39;s consumer electronics gi...   \n",
       "\n",
       "                                                     text  \n",
       "117622  Tension, fear after mall shooting CAMBRIDGE --...  \n",
       "49114   CNN Blog Check this web log throughout the day...  \n",
       "43708   Typhoon Meari Hits South Japan, Triggers Flood...  \n",
       "112859  Three teams in the running to be winter champi...  \n",
       "116500  Samsung, Sony in cross-license deal Two of the...  "
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Import the AG news dataset (same as hw01)\n",
    "#Download them from here \n",
    "!wget https://raw.githubusercontent.com/mhjabreel/CharCnn_Keras/master/data/ag_news_csv/train.csv\n",
    "\n",
    "import pandas as pd\n",
    "import nltk\n",
    "df = pd.read_csv('train.csv')\n",
    "\n",
    "df.columns = [\"label\", \"title\", \"lead\"]\n",
    "label_map = {1:\"world\", 2:\"sport\", 3:\"business\", 4:\"sci/tech\"}\n",
    "def replace_label(x):\n",
    "\treturn label_map[x]\n",
    "df[\"label\"] = df[\"label\"].apply(replace_label) \n",
    "df[\"text\"] = df[\"title\"] + \" \" + df[\"lead\"]\n",
    "df = df.sample(n=10000) # # only use 10K datapoints\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "a49d6b6e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-22T21:40:20.385383Z",
     "start_time": "2022-03-22T21:40:18.447956Z"
    },
    "id": "a49d6b6e"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>title</th>\n",
       "      <th>lead</th>\n",
       "      <th>text</th>\n",
       "      <th>temp_tok</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>117622</th>\n",
       "      <td>business</td>\n",
       "      <td>Tension, fear after mall shooting</td>\n",
       "      <td>CAMBRIDGE -- In a quiet entryway of the Cambri...</td>\n",
       "      <td>Tension, fear after mall shooting CAMBRIDGE --...</td>\n",
       "      <td>tension fear mall shoot cambridge quiet entryw...</td>\n",
       "      <td>[tension, fear, mall, shoot, cambridge, quiet,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49114</th>\n",
       "      <td>sci/tech</td>\n",
       "      <td>CNN Blog</td>\n",
       "      <td>Check this web log throughout the day as CNN A...</td>\n",
       "      <td>CNN Blog Check this web log throughout the day...</td>\n",
       "      <td>cnn blog check web log day cnn anchor space co...</td>\n",
       "      <td>[cnn, blog, check, web, log, day, cnn, anchor,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43708</th>\n",
       "      <td>world</td>\n",
       "      <td>Typhoon Meari Hits South Japan, Triggers Floods</td>\n",
       "      <td>TOKYO (Reuters) - A record eighth typhoon swe...</td>\n",
       "      <td>Typhoon Meari Hits South Japan, Triggers Flood...</td>\n",
       "      <td>typhoon meari hits south japan triggers floods...</td>\n",
       "      <td>[typhoon, meari, hits, south, japan, triggers,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112859</th>\n",
       "      <td>sport</td>\n",
       "      <td>Three teams in the running to be winter champi...</td>\n",
       "      <td>BERLIN: Three teams have the chance to lift th...</td>\n",
       "      <td>Three teams in the running to be winter champi...</td>\n",
       "      <td>team running winter champion germany berlin te...</td>\n",
       "      <td>[team, running, winter, champion, germany, ber...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116500</th>\n",
       "      <td>sci/tech</td>\n",
       "      <td>Samsung, Sony in cross-license deal</td>\n",
       "      <td>Two of the world #39;s consumer electronics gi...</td>\n",
       "      <td>Samsung, Sony in cross-license deal Two of the...</td>\n",
       "      <td>samsung sony cross license deal world 39;s con...</td>\n",
       "      <td>[samsung, sony, cross, license, deal, world, 3...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           label                                              title  \\\n",
       "117622  business                  Tension, fear after mall shooting   \n",
       "49114   sci/tech                                           CNN Blog   \n",
       "43708      world    Typhoon Meari Hits South Japan, Triggers Floods   \n",
       "112859     sport  Three teams in the running to be winter champi...   \n",
       "116500  sci/tech                Samsung, Sony in cross-license deal   \n",
       "\n",
       "                                                     lead  \\\n",
       "117622  CAMBRIDGE -- In a quiet entryway of the Cambri...   \n",
       "49114   Check this web log throughout the day as CNN A...   \n",
       "43708    TOKYO (Reuters) - A record eighth typhoon swe...   \n",
       "112859  BERLIN: Three teams have the chance to lift th...   \n",
       "116500  Two of the world #39;s consumer electronics gi...   \n",
       "\n",
       "                                                     text  \\\n",
       "117622  Tension, fear after mall shooting CAMBRIDGE --...   \n",
       "49114   CNN Blog Check this web log throughout the day...   \n",
       "43708   Typhoon Meari Hits South Japan, Triggers Flood...   \n",
       "112859  Three teams in the running to be winter champi...   \n",
       "116500  Samsung, Sony in cross-license deal Two of the...   \n",
       "\n",
       "                                                 temp_tok  \\\n",
       "117622  tension fear mall shoot cambridge quiet entryw...   \n",
       "49114   cnn blog check web log day cnn anchor space co...   \n",
       "43708   typhoon meari hits south japan triggers floods...   \n",
       "112859  team running winter champion germany berlin te...   \n",
       "116500  samsung sony cross license deal world 39;s con...   \n",
       "\n",
       "                                                   tokens  \n",
       "117622  [tension, fear, mall, shoot, cambridge, quiet,...  \n",
       "49114   [cnn, blog, check, web, log, day, cnn, anchor,...  \n",
       "43708   [typhoon, meari, hits, south, japan, triggers,...  \n",
       "112859  [team, running, winter, champion, germany, ber...  \n",
       "116500  [samsung, sony, cross, license, deal, world, 3...  "
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab = 200\n",
    "##TODO tokenize the text, only keep 200 most frequent words \n",
    "def tokenize(text, return_list = False):\n",
    "    tokens = nlp(text)\n",
    "    if return_list == True:\n",
    "        l_1 = []\n",
    "        for w in tokens:\n",
    "            if not w.is_stop and not w.is_punct and not w.is_digit:\n",
    "                l_1.append(w.lemma_.lower())\n",
    "        return l_1\n",
    "    else:\n",
    "        l = ''\n",
    "        for w in tokens:\n",
    "            if not w.is_stop and not w.is_punct and not w.is_digit:\n",
    "                l += w.lemma_.lower() + ' '\n",
    "        return l \n",
    "\n",
    "#tokenize the data in token col\n",
    "\n",
    "df['temp_tok'] = df['text'].apply(lambda x: tokenize(x))\n",
    "df['tokens'] = df['text'].apply(lambda x: tokenize(x, return_list=True))\n",
    "\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "36564c8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('39;s', 2533), ('say', 2113), ('new', 1789), ('reuters', 1463), ('ap', 1309), ('year', 1253), ('company', 848), ('win', 745), ('world', 739), ('wednesday', 640), ('report', 622), ('monday', 618), ('u.s.', 616), ('tuesday', 611), ('thursday', 610), ('oil', 603), ('game', 602), ('week', 595), ('inc.', 566), ('friday', 550), ('high', 535), ('million', 533), ('york', 529), ('day', 518), ('price', 514), ('iraq', 506), ('kill', 505), ('yesterday', 501), ('plan', 496), ('lead', 493), ('time', 481), ('president', 475), ('end', 470), ('united', 450), ('microsoft', 442), ('group', 438), ('team', 436), ('sunday', 408), ('security', 408), ('government', 406), ('second', 405), ('official', 405), ('afp', 401), ('market', 394), ('percent', 393), ('announce', 390), ('open', 389), ('month', 389), ('stock', 389), ('rise', 388), ('today', 388), ('sale', 386), ('state', 380), ('quot', 377), ('service', 377), ('people', 376), ('profit', 376), ('corp.', 375), ('quarter', 367), ('run', 366), ('set', 363), ('record', 356), ('night', 355), ('season', 352), ('minister', 351), ('software', 346), ('saturday', 344), ('internet', 343), ('big', 342), ('deal', 338), ('take', 337), ('large', 336), ('share', 329), ('china', 328), ('washington', 324), ('business', 318), ('leader', 318), ('victory', 317), ('hold', 317), ('computer', 315), ('country', 312), ('start', 311), ('expect', 309), ('help', 309), ('city', 300), ('attack', 298), ('network', 298), ('court', 295), ('man', 287), ('find', 287), ('release', 285), ('chief', 285), ('=', 282), ('home', 280), ('bush', 279), ('billion', 279), ('come', 278), ('search', 278), ('international', 276), ('test', 274), ('news', 272), ('final', 271), ('late', 271), ('hit', 270), ('play', 264), ('bank', 264), ('force', 263), ('fall', 263), ('election', 262), ('long', 262), ('dollar', 261), ('american', 259), ('cut', 258), ('leave', 256), ('launch', 255), ('european', 255), ('early', 250), ('online', 250), ('talk', 250), ('offer', 249), ('point', 249), ('india', 246), ('technology', 246), ('cup', 245), ('sell', 242), ('close', 242), ('sign', 240), ('charge', 239), ('call', 238), ('player', 235), ('old', 232), ('british', 231), ('google', 230), ('face', 225), ('states', 225), ('space', 224), ('accord', 224), ('research', 223), ('federal', 223), ('industry', 223), ('giant', 222), ('iraqi', 222), ('prime', 222), ('agree', 222), ('coach', 222), ('growth', 220), ('league', 219), ('return', 218), ('give', 218), ('red', 216), ('national', 216), ('phone', 216), ('look', 214), ('australia', 212), ('south', 211), ('japan', 211), ('maker', 211), ('begin', 211), ('buy', 210), ('north', 210), ('police', 209), ('mobile', 209), ('low', 208), ('good', 208), ('major', 207), ('39;t', 207), ('system', 207), ('beat', 207), ('al', 206), ('base', 206), ('include', 205), ('lt;b&gt;', 205), ('&lt;/b&gt', 205), ('london', 204), ('get', 202), ('head', 201), ('lt;a', 201), ('executive', 201), ('music', 201), ('go', 201), ('way', 200), ('loss', 200), ('pay', 199), ('firm', 199), ('drug', 199), ('nation', 198), ('trade', 198), ('work', 198), ('job', 197), ('post', 197), ('union', 197), ('rate', 196), ('bid', 196), ('see', 192), ('ibm', 192), ('air', 192), ('baghdad', 190), ('web', 189), ('increase', 189), ('right', 189)]\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "n=10000\n",
    "\n",
    "txt = ''\n",
    "for i in range(n):\n",
    "    #change to tokens\n",
    "    txt += df['temp_tok'].values[i]\n",
    "\n",
    "split_it = txt.split()\n",
    "Counter = Counter(split_it)  \n",
    "most_occur = Counter.most_common(vocab)\n",
    "  \n",
    "print(most_occur)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "e770f7cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "#create dict to acces keys only\n",
    "dict_mo ={}\n",
    "\n",
    "for k, v in most_occur:\n",
    "    dict_mo.setdefault(k, [v])\n",
    "    \n",
    "\n",
    "def keep_top_200(l1, l2):\n",
    "    l1 = np.array(l1)\n",
    "    l2 = np.array(l2)\n",
    "    mask = np.in1d(l1, l2)\n",
    "    return l1[mask].tolist()\n",
    "\n",
    "df['most_freq_tokens'] = df['tokens'].apply(lambda x: keep_top_200(x, list(dict_mo.keys())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "0a9bf34f",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(most_occur)\n",
    "word2id = {w[0] : i + 1 for i, w in enumerate(most_occur)}\n",
    "word2id\n",
    "\n",
    "word2id['say']\n",
    "\n",
    "def freq_index(l):\n",
    "    l_temp = [0]*200\n",
    "    for t in l:\n",
    "        l_temp[word2id[t]-1] = l_temp[word2id[t]-1] +1\n",
    "    return l_temp\n",
    "\n",
    "df['freq_index'] = df['most_freq_tokens'].apply(lambda x: freq_index(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "615bb5dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>title</th>\n",
       "      <th>lead</th>\n",
       "      <th>text</th>\n",
       "      <th>temp_tok</th>\n",
       "      <th>tokens</th>\n",
       "      <th>most_freq_tokens</th>\n",
       "      <th>freq_index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>117622</th>\n",
       "      <td>business</td>\n",
       "      <td>Tension, fear after mall shooting</td>\n",
       "      <td>CAMBRIDGE -- In a quiet entryway of the Cambri...</td>\n",
       "      <td>Tension, fear after mall shooting CAMBRIDGE --...</td>\n",
       "      <td>tension fear mall shoot cambridge quiet entryw...</td>\n",
       "      <td>[tension, fear, mall, shoot, cambridge, quiet,...</td>\n",
       "      <td>[year, old, say]</td>\n",
       "      <td>[0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49114</th>\n",
       "      <td>sci/tech</td>\n",
       "      <td>CNN Blog</td>\n",
       "      <td>Check this web log throughout the day as CNN A...</td>\n",
       "      <td>CNN Blog Check this web log throughout the day...</td>\n",
       "      <td>cnn blog check web log day cnn anchor space co...</td>\n",
       "      <td>[cnn, blog, check, web, log, day, cnn, anchor,...</td>\n",
       "      <td>[web, day, space, 39;s, second]</td>\n",
       "      <td>[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43708</th>\n",
       "      <td>world</td>\n",
       "      <td>Typhoon Meari Hits South Japan, Triggers Floods</td>\n",
       "      <td>TOKYO (Reuters) - A record eighth typhoon swe...</td>\n",
       "      <td>Typhoon Meari Hits South Japan, Triggers Flood...</td>\n",
       "      <td>typhoon meari hits south japan triggers floods...</td>\n",
       "      <td>[typhoon, meari, hits, south, japan, triggers,...</td>\n",
       "      <td>[south, japan, reuters, record, japan, wednesd...</td>\n",
       "      <td>[0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112859</th>\n",
       "      <td>sport</td>\n",
       "      <td>Three teams in the running to be winter champi...</td>\n",
       "      <td>BERLIN: Three teams have the chance to lift th...</td>\n",
       "      <td>Three teams in the running to be winter champi...</td>\n",
       "      <td>team running winter champion germany berlin te...</td>\n",
       "      <td>[team, running, winter, champion, germany, ber...</td>\n",
       "      <td>[team, team, head, final, take, week]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116500</th>\n",
       "      <td>sci/tech</td>\n",
       "      <td>Samsung, Sony in cross-license deal</td>\n",
       "      <td>Two of the world #39;s consumer electronics gi...</td>\n",
       "      <td>Samsung, Sony in cross-license deal Two of the...</td>\n",
       "      <td>samsung sony cross license deal world 39;s con...</td>\n",
       "      <td>[samsung, sony, cross, license, deal, world, 3...</td>\n",
       "      <td>[deal, world, 39;s, giant, sign, major, company]</td>\n",
       "      <td>[1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           label                                              title  \\\n",
       "117622  business                  Tension, fear after mall shooting   \n",
       "49114   sci/tech                                           CNN Blog   \n",
       "43708      world    Typhoon Meari Hits South Japan, Triggers Floods   \n",
       "112859     sport  Three teams in the running to be winter champi...   \n",
       "116500  sci/tech                Samsung, Sony in cross-license deal   \n",
       "\n",
       "                                                     lead  \\\n",
       "117622  CAMBRIDGE -- In a quiet entryway of the Cambri...   \n",
       "49114   Check this web log throughout the day as CNN A...   \n",
       "43708    TOKYO (Reuters) - A record eighth typhoon swe...   \n",
       "112859  BERLIN: Three teams have the chance to lift th...   \n",
       "116500  Two of the world #39;s consumer electronics gi...   \n",
       "\n",
       "                                                     text  \\\n",
       "117622  Tension, fear after mall shooting CAMBRIDGE --...   \n",
       "49114   CNN Blog Check this web log throughout the day...   \n",
       "43708   Typhoon Meari Hits South Japan, Triggers Flood...   \n",
       "112859  Three teams in the running to be winter champi...   \n",
       "116500  Samsung, Sony in cross-license deal Two of the...   \n",
       "\n",
       "                                                 temp_tok  \\\n",
       "117622  tension fear mall shoot cambridge quiet entryw...   \n",
       "49114   cnn blog check web log day cnn anchor space co...   \n",
       "43708   typhoon meari hits south japan triggers floods...   \n",
       "112859  team running winter champion germany berlin te...   \n",
       "116500  samsung sony cross license deal world 39;s con...   \n",
       "\n",
       "                                                   tokens  \\\n",
       "117622  [tension, fear, mall, shoot, cambridge, quiet,...   \n",
       "49114   [cnn, blog, check, web, log, day, cnn, anchor,...   \n",
       "43708   [typhoon, meari, hits, south, japan, triggers,...   \n",
       "112859  [team, running, winter, champion, germany, ber...   \n",
       "116500  [samsung, sony, cross, license, deal, world, 3...   \n",
       "\n",
       "                                         most_freq_tokens  \\\n",
       "117622                                   [year, old, say]   \n",
       "49114                     [web, day, space, 39;s, second]   \n",
       "43708   [south, japan, reuters, record, japan, wednesd...   \n",
       "112859              [team, team, head, final, take, week]   \n",
       "116500   [deal, world, 39;s, giant, sign, major, company]   \n",
       "\n",
       "                                               freq_index  \n",
       "117622  [0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "49114   [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "43708   [0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, ...  \n",
       "112859  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "116500  [1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, ...  "
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "7502761a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>title</th>\n",
       "      <th>lead</th>\n",
       "      <th>text</th>\n",
       "      <th>temp_tok</th>\n",
       "      <th>tokens</th>\n",
       "      <th>most_freq_tokens</th>\n",
       "      <th>freq_index</th>\n",
       "      <th>freq</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>117622</th>\n",
       "      <td>business</td>\n",
       "      <td>Tension, fear after mall shooting</td>\n",
       "      <td>CAMBRIDGE -- In a quiet entryway of the Cambri...</td>\n",
       "      <td>Tension, fear after mall shooting CAMBRIDGE --...</td>\n",
       "      <td>tension fear mall shoot cambridge quiet entryw...</td>\n",
       "      <td>[tension, fear, mall, shoot, cambridge, quiet,...</td>\n",
       "      <td>[year, old, say]</td>\n",
       "      <td>[0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>year old say</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49114</th>\n",
       "      <td>sci/tech</td>\n",
       "      <td>CNN Blog</td>\n",
       "      <td>Check this web log throughout the day as CNN A...</td>\n",
       "      <td>CNN Blog Check this web log throughout the day...</td>\n",
       "      <td>cnn blog check web log day cnn anchor space co...</td>\n",
       "      <td>[cnn, blog, check, web, log, day, cnn, anchor,...</td>\n",
       "      <td>[web, day, space, 39;s, second]</td>\n",
       "      <td>[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>web day space 39;s second</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43708</th>\n",
       "      <td>world</td>\n",
       "      <td>Typhoon Meari Hits South Japan, Triggers Floods</td>\n",
       "      <td>TOKYO (Reuters) - A record eighth typhoon swe...</td>\n",
       "      <td>Typhoon Meari Hits South Japan, Triggers Flood...</td>\n",
       "      <td>typhoon meari hits south japan triggers floods...</td>\n",
       "      <td>[typhoon, meari, hits, south, japan, triggers,...</td>\n",
       "      <td>[south, japan, reuters, record, japan, wednesd...</td>\n",
       "      <td>[0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>south japan reuters record japan wednesday kil...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112859</th>\n",
       "      <td>sport</td>\n",
       "      <td>Three teams in the running to be winter champi...</td>\n",
       "      <td>BERLIN: Three teams have the chance to lift th...</td>\n",
       "      <td>Three teams in the running to be winter champi...</td>\n",
       "      <td>team running winter champion germany berlin te...</td>\n",
       "      <td>[team, running, winter, champion, germany, ber...</td>\n",
       "      <td>[team, team, head, final, take, week]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>team team head final take week</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116500</th>\n",
       "      <td>sci/tech</td>\n",
       "      <td>Samsung, Sony in cross-license deal</td>\n",
       "      <td>Two of the world #39;s consumer electronics gi...</td>\n",
       "      <td>Samsung, Sony in cross-license deal Two of the...</td>\n",
       "      <td>samsung sony cross license deal world 39;s con...</td>\n",
       "      <td>[samsung, sony, cross, license, deal, world, 3...</td>\n",
       "      <td>[deal, world, 39;s, giant, sign, major, company]</td>\n",
       "      <td>[1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>deal world 39;s giant sign major company</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           label                                              title  \\\n",
       "117622  business                  Tension, fear after mall shooting   \n",
       "49114   sci/tech                                           CNN Blog   \n",
       "43708      world    Typhoon Meari Hits South Japan, Triggers Floods   \n",
       "112859     sport  Three teams in the running to be winter champi...   \n",
       "116500  sci/tech                Samsung, Sony in cross-license deal   \n",
       "\n",
       "                                                     lead  \\\n",
       "117622  CAMBRIDGE -- In a quiet entryway of the Cambri...   \n",
       "49114   Check this web log throughout the day as CNN A...   \n",
       "43708    TOKYO (Reuters) - A record eighth typhoon swe...   \n",
       "112859  BERLIN: Three teams have the chance to lift th...   \n",
       "116500  Two of the world #39;s consumer electronics gi...   \n",
       "\n",
       "                                                     text  \\\n",
       "117622  Tension, fear after mall shooting CAMBRIDGE --...   \n",
       "49114   CNN Blog Check this web log throughout the day...   \n",
       "43708   Typhoon Meari Hits South Japan, Triggers Flood...   \n",
       "112859  Three teams in the running to be winter champi...   \n",
       "116500  Samsung, Sony in cross-license deal Two of the...   \n",
       "\n",
       "                                                 temp_tok  \\\n",
       "117622  tension fear mall shoot cambridge quiet entryw...   \n",
       "49114   cnn blog check web log day cnn anchor space co...   \n",
       "43708   typhoon meari hits south japan triggers floods...   \n",
       "112859  team running winter champion germany berlin te...   \n",
       "116500  samsung sony cross license deal world 39;s con...   \n",
       "\n",
       "                                                   tokens  \\\n",
       "117622  [tension, fear, mall, shoot, cambridge, quiet,...   \n",
       "49114   [cnn, blog, check, web, log, day, cnn, anchor,...   \n",
       "43708   [typhoon, meari, hits, south, japan, triggers,...   \n",
       "112859  [team, running, winter, champion, germany, ber...   \n",
       "116500  [samsung, sony, cross, license, deal, world, 3...   \n",
       "\n",
       "                                         most_freq_tokens  \\\n",
       "117622                                   [year, old, say]   \n",
       "49114                     [web, day, space, 39;s, second]   \n",
       "43708   [south, japan, reuters, record, japan, wednesd...   \n",
       "112859              [team, team, head, final, take, week]   \n",
       "116500   [deal, world, 39;s, giant, sign, major, company]   \n",
       "\n",
       "                                               freq_index  \\\n",
       "117622  [0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "49114   [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "43708   [0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, ...   \n",
       "112859  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "116500  [1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, ...   \n",
       "\n",
       "                                                     freq  \n",
       "117622                                       year old say  \n",
       "49114                           web day space 39;s second  \n",
       "43708   south japan reuters record japan wednesday kil...  \n",
       "112859                     team team head final take week  \n",
       "116500           deal world 39;s giant sign major company  "
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#feels like an unnecessary step... but didnt get it to work without it\n",
    "def to_string(l):\n",
    "    st = ''\n",
    "    for i in l:\n",
    "        st += i + ' '\n",
    "    st = st[:-1]\n",
    "    return st\n",
    "df['freq'] = df['most_freq_tokens'].apply(lambda x: to_string(x))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "ecc981a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 1)"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#create data for encoder\n",
    "\n",
    "X = np.array([st for st in df.freq]).reshape(-1,1)\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "c4c0f840",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-22T21:40:23.322875Z",
     "start_time": "2022-03-22T21:40:23.311923Z"
    },
    "id": "c4c0f840"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 9730)\n",
      "(10000, 100)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "length = 100\n",
    "#TODO create a one_hot representation for each word and truncate/pad the sequences such that they are all of the \n",
    "#same length (here we use 100)\n",
    "\n",
    "#encoding\n",
    "onehot_encoder = OneHotEncoder(sparse=False)\n",
    "\n",
    "onehot_encoded = onehot_encoder.fit_transform(X)\n",
    "print(onehot_encoded.shape)\n",
    "\n",
    "#truncate\n",
    "print(onehot_encoded[:,:100].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "c3d193dd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-22T21:40:28.364553Z",
     "start_time": "2022-03-22T21:40:28.354695Z"
    },
    "id": "c3d193dd"
   },
   "outputs": [],
   "source": [
    "##TODO create your torch embedding like we did in notebook 5! (hint: predicting labels: world, sport, business, \n",
    "#and sci/tech)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "9d203068",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>title</th>\n",
       "      <th>lead</th>\n",
       "      <th>text</th>\n",
       "      <th>temp_tok</th>\n",
       "      <th>tokens</th>\n",
       "      <th>most_freq_tokens</th>\n",
       "      <th>freq_index</th>\n",
       "      <th>freq</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>117622</th>\n",
       "      <td>business</td>\n",
       "      <td>Tension, fear after mall shooting</td>\n",
       "      <td>CAMBRIDGE -- In a quiet entryway of the Cambri...</td>\n",
       "      <td>Tension, fear after mall shooting CAMBRIDGE --...</td>\n",
       "      <td>tension fear mall shoot cambridge quiet entryw...</td>\n",
       "      <td>[tension, fear, mall, shoot, cambridge, quiet,...</td>\n",
       "      <td>[year, old, say]</td>\n",
       "      <td>[0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>year old say</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49114</th>\n",
       "      <td>sci/tech</td>\n",
       "      <td>CNN Blog</td>\n",
       "      <td>Check this web log throughout the day as CNN A...</td>\n",
       "      <td>CNN Blog Check this web log throughout the day...</td>\n",
       "      <td>cnn blog check web log day cnn anchor space co...</td>\n",
       "      <td>[cnn, blog, check, web, log, day, cnn, anchor,...</td>\n",
       "      <td>[web, day, space, 39;s, second]</td>\n",
       "      <td>[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>web day space 39;s second</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43708</th>\n",
       "      <td>world</td>\n",
       "      <td>Typhoon Meari Hits South Japan, Triggers Floods</td>\n",
       "      <td>TOKYO (Reuters) - A record eighth typhoon swe...</td>\n",
       "      <td>Typhoon Meari Hits South Japan, Triggers Flood...</td>\n",
       "      <td>typhoon meari hits south japan triggers floods...</td>\n",
       "      <td>[typhoon, meari, hits, south, japan, triggers,...</td>\n",
       "      <td>[south, japan, reuters, record, japan, wednesd...</td>\n",
       "      <td>[0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>south japan reuters record japan wednesday kil...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112859</th>\n",
       "      <td>sport</td>\n",
       "      <td>Three teams in the running to be winter champi...</td>\n",
       "      <td>BERLIN: Three teams have the chance to lift th...</td>\n",
       "      <td>Three teams in the running to be winter champi...</td>\n",
       "      <td>team running winter champion germany berlin te...</td>\n",
       "      <td>[team, running, winter, champion, germany, ber...</td>\n",
       "      <td>[team, team, head, final, take, week]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>team team head final take week</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116500</th>\n",
       "      <td>sci/tech</td>\n",
       "      <td>Samsung, Sony in cross-license deal</td>\n",
       "      <td>Two of the world #39;s consumer electronics gi...</td>\n",
       "      <td>Samsung, Sony in cross-license deal Two of the...</td>\n",
       "      <td>samsung sony cross license deal world 39;s con...</td>\n",
       "      <td>[samsung, sony, cross, license, deal, world, 3...</td>\n",
       "      <td>[deal, world, 39;s, giant, sign, major, company]</td>\n",
       "      <td>[1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>deal world 39;s giant sign major company</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           label                                              title  \\\n",
       "117622  business                  Tension, fear after mall shooting   \n",
       "49114   sci/tech                                           CNN Blog   \n",
       "43708      world    Typhoon Meari Hits South Japan, Triggers Floods   \n",
       "112859     sport  Three teams in the running to be winter champi...   \n",
       "116500  sci/tech                Samsung, Sony in cross-license deal   \n",
       "\n",
       "                                                     lead  \\\n",
       "117622  CAMBRIDGE -- In a quiet entryway of the Cambri...   \n",
       "49114   Check this web log throughout the day as CNN A...   \n",
       "43708    TOKYO (Reuters) - A record eighth typhoon swe...   \n",
       "112859  BERLIN: Three teams have the chance to lift th...   \n",
       "116500  Two of the world #39;s consumer electronics gi...   \n",
       "\n",
       "                                                     text  \\\n",
       "117622  Tension, fear after mall shooting CAMBRIDGE --...   \n",
       "49114   CNN Blog Check this web log throughout the day...   \n",
       "43708   Typhoon Meari Hits South Japan, Triggers Flood...   \n",
       "112859  Three teams in the running to be winter champi...   \n",
       "116500  Samsung, Sony in cross-license deal Two of the...   \n",
       "\n",
       "                                                 temp_tok  \\\n",
       "117622  tension fear mall shoot cambridge quiet entryw...   \n",
       "49114   cnn blog check web log day cnn anchor space co...   \n",
       "43708   typhoon meari hits south japan triggers floods...   \n",
       "112859  team running winter champion germany berlin te...   \n",
       "116500  samsung sony cross license deal world 39;s con...   \n",
       "\n",
       "                                                   tokens  \\\n",
       "117622  [tension, fear, mall, shoot, cambridge, quiet,...   \n",
       "49114   [cnn, blog, check, web, log, day, cnn, anchor,...   \n",
       "43708   [typhoon, meari, hits, south, japan, triggers,...   \n",
       "112859  [team, running, winter, champion, germany, ber...   \n",
       "116500  [samsung, sony, cross, license, deal, world, 3...   \n",
       "\n",
       "                                         most_freq_tokens  \\\n",
       "117622                                   [year, old, say]   \n",
       "49114                     [web, day, space, 39;s, second]   \n",
       "43708   [south, japan, reuters, record, japan, wednesd...   \n",
       "112859              [team, team, head, final, take, week]   \n",
       "116500   [deal, world, 39;s, giant, sign, major, company]   \n",
       "\n",
       "                                               freq_index  \\\n",
       "117622  [0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "49114   [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "43708   [0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, ...   \n",
       "112859  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "116500  [1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, ...   \n",
       "\n",
       "                                                     freq  \n",
       "117622                                       year old say  \n",
       "49114                           web day space 39;s second  \n",
       "43708   south japan reuters record japan wednesday kil...  \n",
       "112859                     team team head final take week  \n",
       "116500           deal world 39;s giant sign major company  "
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#create an index with the most common words\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "d4076fb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 200) (10000,)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>title</th>\n",
       "      <th>lead</th>\n",
       "      <th>text</th>\n",
       "      <th>temp_tok</th>\n",
       "      <th>tokens</th>\n",
       "      <th>most_freq_tokens</th>\n",
       "      <th>freq_index</th>\n",
       "      <th>freq</th>\n",
       "      <th>label_nr</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>117622</th>\n",
       "      <td>business</td>\n",
       "      <td>Tension, fear after mall shooting</td>\n",
       "      <td>CAMBRIDGE -- In a quiet entryway of the Cambri...</td>\n",
       "      <td>Tension, fear after mall shooting CAMBRIDGE --...</td>\n",
       "      <td>tension fear mall shoot cambridge quiet entryw...</td>\n",
       "      <td>[tension, fear, mall, shoot, cambridge, quiet,...</td>\n",
       "      <td>[year, old, say]</td>\n",
       "      <td>[0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>year old say</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49114</th>\n",
       "      <td>sci/tech</td>\n",
       "      <td>CNN Blog</td>\n",
       "      <td>Check this web log throughout the day as CNN A...</td>\n",
       "      <td>CNN Blog Check this web log throughout the day...</td>\n",
       "      <td>cnn blog check web log day cnn anchor space co...</td>\n",
       "      <td>[cnn, blog, check, web, log, day, cnn, anchor,...</td>\n",
       "      <td>[web, day, space, 39;s, second]</td>\n",
       "      <td>[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>web day space 39;s second</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43708</th>\n",
       "      <td>world</td>\n",
       "      <td>Typhoon Meari Hits South Japan, Triggers Floods</td>\n",
       "      <td>TOKYO (Reuters) - A record eighth typhoon swe...</td>\n",
       "      <td>Typhoon Meari Hits South Japan, Triggers Flood...</td>\n",
       "      <td>typhoon meari hits south japan triggers floods...</td>\n",
       "      <td>[typhoon, meari, hits, south, japan, triggers,...</td>\n",
       "      <td>[south, japan, reuters, record, japan, wednesd...</td>\n",
       "      <td>[0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>south japan reuters record japan wednesday kil...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112859</th>\n",
       "      <td>sport</td>\n",
       "      <td>Three teams in the running to be winter champi...</td>\n",
       "      <td>BERLIN: Three teams have the chance to lift th...</td>\n",
       "      <td>Three teams in the running to be winter champi...</td>\n",
       "      <td>team running winter champion germany berlin te...</td>\n",
       "      <td>[team, running, winter, champion, germany, ber...</td>\n",
       "      <td>[team, team, head, final, take, week]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>team team head final take week</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116500</th>\n",
       "      <td>sci/tech</td>\n",
       "      <td>Samsung, Sony in cross-license deal</td>\n",
       "      <td>Two of the world #39;s consumer electronics gi...</td>\n",
       "      <td>Samsung, Sony in cross-license deal Two of the...</td>\n",
       "      <td>samsung sony cross license deal world 39;s con...</td>\n",
       "      <td>[samsung, sony, cross, license, deal, world, 3...</td>\n",
       "      <td>[deal, world, 39;s, giant, sign, major, company]</td>\n",
       "      <td>[1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>deal world 39;s giant sign major company</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           label                                              title  \\\n",
       "117622  business                  Tension, fear after mall shooting   \n",
       "49114   sci/tech                                           CNN Blog   \n",
       "43708      world    Typhoon Meari Hits South Japan, Triggers Floods   \n",
       "112859     sport  Three teams in the running to be winter champi...   \n",
       "116500  sci/tech                Samsung, Sony in cross-license deal   \n",
       "\n",
       "                                                     lead  \\\n",
       "117622  CAMBRIDGE -- In a quiet entryway of the Cambri...   \n",
       "49114   Check this web log throughout the day as CNN A...   \n",
       "43708    TOKYO (Reuters) - A record eighth typhoon swe...   \n",
       "112859  BERLIN: Three teams have the chance to lift th...   \n",
       "116500  Two of the world #39;s consumer electronics gi...   \n",
       "\n",
       "                                                     text  \\\n",
       "117622  Tension, fear after mall shooting CAMBRIDGE --...   \n",
       "49114   CNN Blog Check this web log throughout the day...   \n",
       "43708   Typhoon Meari Hits South Japan, Triggers Flood...   \n",
       "112859  Three teams in the running to be winter champi...   \n",
       "116500  Samsung, Sony in cross-license deal Two of the...   \n",
       "\n",
       "                                                 temp_tok  \\\n",
       "117622  tension fear mall shoot cambridge quiet entryw...   \n",
       "49114   cnn blog check web log day cnn anchor space co...   \n",
       "43708   typhoon meari hits south japan triggers floods...   \n",
       "112859  team running winter champion germany berlin te...   \n",
       "116500  samsung sony cross license deal world 39;s con...   \n",
       "\n",
       "                                                   tokens  \\\n",
       "117622  [tension, fear, mall, shoot, cambridge, quiet,...   \n",
       "49114   [cnn, blog, check, web, log, day, cnn, anchor,...   \n",
       "43708   [typhoon, meari, hits, south, japan, triggers,...   \n",
       "112859  [team, running, winter, champion, germany, ber...   \n",
       "116500  [samsung, sony, cross, license, deal, world, 3...   \n",
       "\n",
       "                                         most_freq_tokens  \\\n",
       "117622                                   [year, old, say]   \n",
       "49114                     [web, day, space, 39;s, second]   \n",
       "43708   [south, japan, reuters, record, japan, wednesd...   \n",
       "112859              [team, team, head, final, take, week]   \n",
       "116500   [deal, world, 39;s, giant, sign, major, company]   \n",
       "\n",
       "                                               freq_index  \\\n",
       "117622  [0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "49114   [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "43708   [0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, ...   \n",
       "112859  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "116500  [1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, ...   \n",
       "\n",
       "                                                     freq  label_nr  \n",
       "117622                                       year old say         2  \n",
       "49114                           web day space 39;s second         3  \n",
       "43708   south japan reuters record japan wednesday kil...         0  \n",
       "112859                     team team head final take week         1  \n",
       "116500           deal world 39;s giant sign major company         3  "
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#X data and y data\n",
    "features = np.array([l for l in df.freq_index])\n",
    "\n",
    "def label_trans(l):\n",
    "    if l == 'world':\n",
    "        return 0\n",
    "    if l == 'sport':\n",
    "        return 1\n",
    "    if l == 'business':\n",
    "        return 2\n",
    "    if l == 'sci/tech':\n",
    "        return 3\n",
    "    \n",
    "    \n",
    "df['label_nr'] = df['label'].apply(lambda x: label_trans(x))\n",
    "\n",
    "y = np.array([i for i in df.label_nr])\n",
    "\n",
    "print(features.shape, y.shape)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "24950de8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0]), 2)\n"
     ]
    }
   ],
   "source": [
    "class GenericDataset(Dataset):\n",
    "\n",
    "  def __init__(self, X, y):\n",
    "    self.X = X\n",
    "    self.y = y\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.y)\n",
    "\n",
    "  def __getitem__(self, index):\n",
    "    return self.X[index], self.y[index]\n",
    "\n",
    "dataset = GenericDataset(features, y)\n",
    "for d in dataset:\n",
    "    print(d)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "2cac3bd2",
   "metadata": {
    "id": "2cac3bd2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EmbeddingNet(\n",
      "  (embedding): Embedding(201, 2)\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (fc1): Linear(in_features=400, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=1, bias=True)\n",
      "  (softmax): Softmax(dim=None)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "#fattar inte riktigt embedd...\n",
    "\n",
    "# Model setup\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class EmbeddingNet(nn.Module):\n",
    "  def __init__(self, num_words=200):\n",
    "    super(EmbeddingNet, self).__init__()\n",
    "    self.embedding = nn.Embedding(num_words + 1, 2)\n",
    "    self.flatten = nn.Flatten()\n",
    "    self.fc1 = nn.Linear(2 * 200, 50)\n",
    "    self.fc2 = nn.Linear(50, 1)\n",
    "    self.softmax = nn.Softmax()\n",
    "\n",
    "  def forward(self, x):\n",
    "    x = self.embedding(x)\n",
    "    x = self.flatten(x)\n",
    "    x = self.fc1(x)\n",
    "    x = self.fc2(x)\n",
    "    x = self.softmax(x)\n",
    "    return x\n",
    "\n",
    "num_words = 200\n",
    "model = EmbeddingNet(num_words)\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "0e60bf57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show the vectors\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "num_words = 200\n",
    "\n",
    "\n",
    "model = EmbeddingNet(num_words)\n",
    "#loss func\n",
    "criterion = nn.BCELoss()\n",
    "#standard optimizer\n",
    "\n",
    "#initialize data \n",
    "loader = torch.utils.data.DataLoader(dataset, batch_size=100, shuffle=True)\n",
    "\n",
    "for i in range(10):\n",
    "#weird step but letx keep it\n",
    "  if i > 0:\n",
    "    for data, label in loader:\n",
    "      optimizer.zero_grad()\n",
    "      outputs = model(data)\n",
    "      loss = criterion(outputs, label.float().unsqueeze(1))\n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "\n",
    "#could predict on new data now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "12c53d05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-44., grad_fn=<BinaryCrossEntropyBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f97a7dfd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
