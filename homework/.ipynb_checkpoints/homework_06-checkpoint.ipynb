{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "present-brown",
   "metadata": {
    "id": "present-brown"
   },
   "source": [
    "# HW06: Transformers and Doc Embeddings\n",
    "\n",
    "Remember that these homework work as a completion grade. **You can skip one section of this homework.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "irish-ending",
   "metadata": {
    "id": "irish-ending",
    "outputId": "ed437d6f-ded0-4840-cfc1-d7e0b676527f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2023-04-03 12:01:35--  https://raw.githubusercontent.com/mhjabreel/CharCnn_Keras/master/data/ag_news_csv/train.csv\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.111.133, 185.199.109.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 29470338 (28M) [text/plain]\n",
      "Saving to: 'train.csv'\n",
      "\n",
      "train.csv           100%[===================>]  28.10M  4.92MB/s    in 6.3s    \n",
      "\n",
      "2023-04-03 12:01:45 (4.48 MB/s) - 'train.csv' saved [29470338/29470338]\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>title</th>\n",
       "      <th>lead</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>52083</th>\n",
       "      <td>sport</td>\n",
       "      <td>Davenport Powers Past Molik, Dementieva Falls</td>\n",
       "      <td>FILDERSTADT, Germany (Reuters) - Lindsay Dave...</td>\n",
       "      <td>Davenport Powers Past Molik, Dementieva Falls ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34647</th>\n",
       "      <td>sci/tech</td>\n",
       "      <td>Apple iMac G5</td>\n",
       "      <td>1.8-GHz PowerPC G5 processor, 512MB DDR SDRAM,...</td>\n",
       "      <td>Apple iMac G5 1.8-GHz PowerPC G5 processor, 51...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29757</th>\n",
       "      <td>world</td>\n",
       "      <td>A Beslan mother #39;s impossible choice</td>\n",
       "      <td>After more than 24 hours in captivity in Besla...</td>\n",
       "      <td>A Beslan mother #39;s impossible choice After ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112012</th>\n",
       "      <td>business</td>\n",
       "      <td>US mobile giants in merger talks</td>\n",
       "      <td>US mobile operators Sprint and Nextel are in t...</td>\n",
       "      <td>US mobile giants in merger talks US mobile ope...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98506</th>\n",
       "      <td>sport</td>\n",
       "      <td>Sorenstam Ends the Year With Trophy (AP)</td>\n",
       "      <td>AP - Despite 33 victories in the last four yea...</td>\n",
       "      <td>Sorenstam Ends the Year With Trophy (AP) AP - ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           label                                          title  \\\n",
       "52083      sport  Davenport Powers Past Molik, Dementieva Falls   \n",
       "34647   sci/tech                                  Apple iMac G5   \n",
       "29757      world        A Beslan mother #39;s impossible choice   \n",
       "112012  business               US mobile giants in merger talks   \n",
       "98506      sport       Sorenstam Ends the Year With Trophy (AP)   \n",
       "\n",
       "                                                     lead  \\\n",
       "52083    FILDERSTADT, Germany (Reuters) - Lindsay Dave...   \n",
       "34647   1.8-GHz PowerPC G5 processor, 512MB DDR SDRAM,...   \n",
       "29757   After more than 24 hours in captivity in Besla...   \n",
       "112012  US mobile operators Sprint and Nextel are in t...   \n",
       "98506   AP - Despite 33 victories in the last four yea...   \n",
       "\n",
       "                                                     text  \n",
       "52083   Davenport Powers Past Molik, Dementieva Falls ...  \n",
       "34647   Apple iMac G5 1.8-GHz PowerPC G5 processor, 51...  \n",
       "29757   A Beslan mother #39;s impossible choice After ...  \n",
       "112012  US mobile giants in merger talks US mobile ope...  \n",
       "98506   Sorenstam Ends the Year With Trophy (AP) AP - ...  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!wget https://raw.githubusercontent.com/mhjabreel/CharCnn_Keras/master/data/ag_news_csv/train.csv\n",
    "\n",
    "import pandas as pd\n",
    "import nltk\n",
    "df = pd.read_csv('train.csv')\n",
    "\n",
    "df.columns = [\"label\", \"title\", \"lead\"]\n",
    "label_map = {1:\"world\", 2:\"sport\", 3:\"business\", 4:\"sci/tech\"}\n",
    "def replace_label(x):\n",
    "    return label_map[x]\n",
    "df[\"label\"] = df[\"label\"].apply(replace_label) \n",
    "df[\"text\"] = df[\"title\"] + \" \" + df[\"lead\"]\n",
    "df = df.sample(n=10000) # # only use 10K datapoints\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "regulated-klein",
   "metadata": {
    "id": "regulated-klein"
   },
   "source": [
    "## Hugginface Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "11d0ab41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>title</th>\n",
       "      <th>lead</th>\n",
       "      <th>text</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>52083</th>\n",
       "      <td>sport</td>\n",
       "      <td>Davenport Powers Past Molik, Dementieva Falls</td>\n",
       "      <td>FILDERSTADT, Germany (Reuters) - Lindsay Dave...</td>\n",
       "      <td>Davenport Powers Past Molik, Dementieva Falls ...</td>\n",
       "      <td>[0, 0, 1, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34647</th>\n",
       "      <td>sci/tech</td>\n",
       "      <td>Apple iMac G5</td>\n",
       "      <td>1.8-GHz PowerPC G5 processor, 512MB DDR SDRAM,...</td>\n",
       "      <td>Apple iMac G5 1.8-GHz PowerPC G5 processor, 51...</td>\n",
       "      <td>[0, 1, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29757</th>\n",
       "      <td>world</td>\n",
       "      <td>A Beslan mother #39;s impossible choice</td>\n",
       "      <td>After more than 24 hours in captivity in Besla...</td>\n",
       "      <td>A Beslan mother #39;s impossible choice After ...</td>\n",
       "      <td>[0, 0, 0, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112012</th>\n",
       "      <td>business</td>\n",
       "      <td>US mobile giants in merger talks</td>\n",
       "      <td>US mobile operators Sprint and Nextel are in t...</td>\n",
       "      <td>US mobile giants in merger talks US mobile ope...</td>\n",
       "      <td>[1, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98506</th>\n",
       "      <td>sport</td>\n",
       "      <td>Sorenstam Ends the Year With Trophy (AP)</td>\n",
       "      <td>AP - Despite 33 victories in the last four yea...</td>\n",
       "      <td>Sorenstam Ends the Year With Trophy (AP) AP - ...</td>\n",
       "      <td>[0, 0, 1, 0]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           label                                          title  \\\n",
       "52083      sport  Davenport Powers Past Molik, Dementieva Falls   \n",
       "34647   sci/tech                                  Apple iMac G5   \n",
       "29757      world        A Beslan mother #39;s impossible choice   \n",
       "112012  business               US mobile giants in merger talks   \n",
       "98506      sport       Sorenstam Ends the Year With Trophy (AP)   \n",
       "\n",
       "                                                     lead  \\\n",
       "52083    FILDERSTADT, Germany (Reuters) - Lindsay Dave...   \n",
       "34647   1.8-GHz PowerPC G5 processor, 512MB DDR SDRAM,...   \n",
       "29757   After more than 24 hours in captivity in Besla...   \n",
       "112012  US mobile operators Sprint and Nextel are in t...   \n",
       "98506   AP - Despite 33 victories in the last four yea...   \n",
       "\n",
       "                                                     text        labels  \n",
       "52083   Davenport Powers Past Molik, Dementieva Falls ...  [0, 0, 1, 0]  \n",
       "34647   Apple iMac G5 1.8-GHz PowerPC G5 processor, 51...  [0, 1, 0, 0]  \n",
       "29757   A Beslan mother #39;s impossible choice After ...  [0, 0, 0, 1]  \n",
       "112012  US mobile giants in merger talks US mobile ope...  [1, 0, 0, 0]  \n",
       "98506   Sorenstam Ends the Year With Trophy (AP) AP - ...  [0, 0, 1, 0]  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#prepare the data and split into training and test\n",
    "import numpy as np\n",
    "\n",
    "labels = np.sort(df.label.unique())\n",
    "\n",
    "#perhaps should use floats here...\n",
    "def one_hot_encoding(x, labels):\n",
    "    return [int(l==x) for l in labels]\n",
    "\n",
    "df['labels'] = df['label'].apply(lambda x: one_hot_encoding(x, labels))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "b3a20c7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "##TODO split the sample into a training and a test set \n",
    "##TODO prepare the dataset for torch.\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "#tokenize the text\n",
    "#ev check len here, ev byta format, could use padding\n",
    "tokenizer = AutoTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "df['tokenized'] = df['text'].apply(lambda x: tokenizer(x, return_tensors='pt', truncation=True, padding=True))\n",
    "\n",
    "#split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['tokenized'], df['labels'], test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "bc4a35bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118159    [input_ids, attention_mask]\n",
      "9006      [input_ids, attention_mask]\n",
      "104343    [input_ids, attention_mask]\n",
      "108450    [input_ids, attention_mask]\n",
      "107612    [input_ids, attention_mask]\n",
      "Name: tokenized, dtype: object 118159    [0, 0, 0, 1]\n",
      "9006      [0, 0, 0, 1]\n",
      "104343    [1, 0, 0, 0]\n",
      "108450    [0, 0, 1, 0]\n",
      "107612    [0, 1, 0, 0]\n",
      "              ...     \n",
      "101358    [0, 0, 1, 0]\n",
      "3928      [1, 0, 0, 0]\n",
      "110807    [0, 0, 0, 1]\n",
      "35216     [0, 1, 0, 0]\n",
      "87395     [0, 0, 0, 1]\n",
      "Name: labels, Length: 7000, dtype: object\n"
     ]
    }
   ],
   "source": [
    "#doesn't feel right\n",
    "print(X_train.head(), y_train)\n",
    "\n",
    "\n",
    "#check for tensor\n",
    "list(df['tokenized'].iloc[[1]])\n",
    "\n",
    "#len seems to be 50 for all\n",
    "len(list(df.tokenized[:30])[0].input_ids[0])\n",
    "\n",
    "#perhaps have to convert training data\n",
    "X_train = X_train.tolist()\n",
    "X_test = X_test.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "5b1acb67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "#be able to move tensor to device\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print (device)\n",
    "\n",
    "#device = torch.device('cuda' if torch.cuda.is_available() else 'cpu') for mac..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "b3471af0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_projector.bias', 'vocab_projector.weight', 'vocab_layer_norm.weight', 'vocab_transform.weight', 'vocab_layer_norm.bias', 'vocab_transform.bias']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.bias', 'classifier.weight', 'pre_classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DistilBertForSequenceClassification(\n",
      "  (distilbert): DistilBertModel(\n",
      "    (embeddings): Embeddings(\n",
      "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
      "      (position_embeddings): Embedding(512, 768)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (transformer): Transformer(\n",
      "      (layer): ModuleList(\n",
      "        (0): TransformerBlock(\n",
      "          (attention): MultiHeadSelfAttention(\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (ffn): FFN(\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (activation): GELUActivation()\n",
      "          )\n",
      "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        )\n",
      "        (1): TransformerBlock(\n",
      "          (attention): MultiHeadSelfAttention(\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (ffn): FFN(\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (activation): GELUActivation()\n",
      "          )\n",
      "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        )\n",
      "        (2): TransformerBlock(\n",
      "          (attention): MultiHeadSelfAttention(\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (ffn): FFN(\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (activation): GELUActivation()\n",
      "          )\n",
      "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        )\n",
      "        (3): TransformerBlock(\n",
      "          (attention): MultiHeadSelfAttention(\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (ffn): FFN(\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (activation): GELUActivation()\n",
      "          )\n",
      "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        )\n",
      "        (4): TransformerBlock(\n",
      "          (attention): MultiHeadSelfAttention(\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (ffn): FFN(\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (activation): GELUActivation()\n",
      "          )\n",
      "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        )\n",
      "        (5): TransformerBlock(\n",
      "          (attention): MultiHeadSelfAttention(\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (ffn): FFN(\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (activation): GELUActivation()\n",
      "          )\n",
      "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (classifier): Linear(in_features=768, out_features=4, bias=True)\n",
      "  (dropout): Dropout(p=0.2, inplace=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "#defining our model\n",
    "from transformers import DistilBertForSequenceClassification, DistilBertConfig, AdamW\n",
    "import torch\n",
    "\n",
    "#can provide more args but should enable saving a trained model, perhaps can remove this\n",
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\"test-trainer\")\n",
    "\n",
    "#creating a simple model, could increase dropout from 0.1 to 0.2, skipping config for now\n",
    "#configuration = DistilBertConfig(output_hidden_states=True, dropout=0.2)\n",
    "\n",
    "#initializing model, cant specify num_labels but write multilabel or switch to automodel\n",
    "model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=4)\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "ea5c1226",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 4])"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#testing some dimensions on torch\n",
    "test_list = [0, 0, 0, 1]\n",
    "test_torch = torch.FloatTensor(test_list).reshape(1,4)\n",
    "test_torch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "47897a8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████| 7000/7000 [1:32:13<00:00,  1.27it/s]\n"
     ]
    }
   ],
   "source": [
    "##TODO fit the model and print the obtained accuracy (hint: you can follow the training steps in the notebook. \n",
    "#To learn more, checkout the trainer class of huggingface transformers)\n",
    "\n",
    "#training the model\n",
    "#neat output\n",
    "from tqdm import tqdm\n",
    "\n",
    "#only cpu seemed available...\n",
    "model.to(device)\n",
    "\n",
    "nr_epochs = 1\n",
    "\n",
    "#setting model to train\n",
    "model.train()\n",
    "\n",
    "#specify optimizer in order to backprop later - taking same as in notebook, could simplify this one\n",
    "optimizer = torch.optim.Adam([\n",
    "    {'params': model.distilbert.parameters(), 'lr': 1e-5},  \n",
    "    {'params': model.classifier.parameters(), 'lr': 1e-3}\n",
    "])\n",
    "\n",
    "for epoch in range(nr_epochs):\n",
    "    for text, label in tqdm(zip(X_train, y_train), total=len(X_train)): #for display\n",
    "        text = text.to(device)\n",
    "        label = torch.FloatTensor(label).reshape(1,4).to(device) #change format of labels\n",
    "        pred = model(**text, labels=label) #diff len perhaps explains **\n",
    "        loss = pred[0]\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        #start fresh\n",
    "        optimizer.zero_grad()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "piano-compound",
   "metadata": {
    "id": "piano-compound"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0000, 0.5400, 0.3000, 0.0100]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[0.0, 0.5400000214576721, 0.30000001192092896, 0.009999999776482582]]"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#trying to grasp the output\n",
    "test_tensor = torch.Tensor([0,0.54,0.3,0.01]).reshape(1,4)\n",
    "print(test_tensor)\n",
    "type(test_tensor.argmax().tolist())\n",
    "max(test_list)\n",
    "test_tensor.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "statistical-recommendation",
   "metadata": {
    "id": "statistical-recommendation"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 3000/3000 [02:17<00:00, 21.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy 0.8976666666666666\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.78      0.85       808\n",
      "           1       0.81      0.94      0.87       706\n",
      "           2       0.93      0.99      0.96       733\n",
      "           3       0.92      0.90      0.91       753\n",
      "\n",
      "    accuracy                           0.90      3000\n",
      "   macro avg       0.90      0.90      0.90      3000\n",
      "weighted avg       0.90      0.90      0.90      3000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "##TODO build a transformer model to do sequence classification with the goal to predict the label from the text\n",
    "\n",
    "#lets try to see how the model does on the test data\n",
    "model.eval()\n",
    "\n",
    "#initializing predictions\n",
    "predictions, labels = [], []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for text, label in tqdm(zip(X_test,y_test), total=len(X_test)): #could avoid this if i didnt do multilabel\n",
    "        pred = model(**text)\n",
    "        labels.append(torch.Tensor(label).argmax().tolist())\n",
    "        predictions.append(pred[0].argmax().tolist())\n",
    "        \n",
    "#taking the same as in the notebook provided\n",
    "from sklearn import metrics\n",
    "accuracy = metrics.accuracy_score(labels, predictions)\n",
    "print (\"accuracy\", accuracy)\n",
    "classification_report = metrics.classification_report(labels, predictions)\n",
    "print (classification_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7fe3a17",
   "metadata": {
    "id": "e7fe3a17"
   },
   "source": [
    "# Doc Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b41f71c",
   "metadata": {
    "id": "3b41f71c"
   },
   "outputs": [],
   "source": [
    "# obtain the data\n",
    "!wget http://alt.qcri.org/semeval2017/task1/data/uploads/sts2017.eval.v1.1.zip\n",
    "!wget http://alt.qcri.org/semeval2017/task1/data/uploads/sts2017.gs.zip\n",
    "\n",
    "!unzip sts2017.eval.v1.1.zip \n",
    "!unzip sts2017.gs.zip "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d48a11ab",
   "metadata": {
    "id": "d48a11ab"
   },
   "outputs": [],
   "source": [
    "# load the data\n",
    "\n",
    "def load_STS_data():\n",
    "    with open(\"STS2017.gs/STS.gs.track5.en-en.txt\") as f:\n",
    "        labels = [float(line.strip()) for line in f]\n",
    "    \n",
    "    text_a, text_b = [], []\n",
    "    with open(\"STS2017.eval.v1.1/STS.input.track5.en-en.txt\") as f:\n",
    "        for line in f:\n",
    "            line = line.strip().split(\"\\t\")\n",
    "            text_a.append(line[0])\n",
    "            text_b.append(line[1])\n",
    "    return text_a, text_b, labels\n",
    "\n",
    "text_a, text_b, labels = load_STS_data()\n",
    "text_a[0], text_b[0], labels[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dee8bcb4",
   "metadata": {
    "id": "dee8bcb4"
   },
   "outputs": [],
   "source": [
    "# some utils\n",
    "from scipy.stats import spearmanr\n",
    "def evaluate(predictions, labels):\n",
    "    print (\"spearman's rank correlation\", spearmanr(predictions, labels)[0])\n",
    "\n",
    "import numpy as np\n",
    "from numpy import dot\n",
    "from numpy.linalg import norm\n",
    "\n",
    "def cosine_similarity(a,b):\n",
    "    return dot(a, b)/(norm(a)*norm(b))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46f02f97",
   "metadata": {
    "id": "46f02f97"
   },
   "outputs": [],
   "source": [
    "# Wordcounts baseline\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vec = CountVectorizer()\n",
    "vec.fit(text_a + text_b)\n",
    "\n",
    "# encode documents\n",
    "text_a_encoded = np.array(vec.transform(text_a).todense())\n",
    "text_b_encoded = np.array(vec.transform(text_b).todense())\n",
    "\n",
    "# predict cosine similarities\n",
    "predictions = [cosine_similarity(a,b) for a,b in zip(text_a_encoded, text_b_encoded)]\n",
    "\n",
    "# evaluate\n",
    "evaluate(predictions, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "965242a5",
   "metadata": {
    "id": "965242a5"
   },
   "outputs": [],
   "source": [
    "##TODO train Doc2Vec on the texts in the dataset\n",
    "##TODO derive the word vectors for each text in the dataset\n",
    "##TODO compute cosine similarity between the text pairs and evaluate spearman's rank correlation\n",
    "## Don't worry if results are not satisfactory using Doc2Vec (the dataset is too small to train good embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e67b67c8",
   "metadata": {
    "id": "e67b67c8"
   },
   "outputs": [],
   "source": [
    "##TODO do the same with embeddings provided by spaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cf40ced",
   "metadata": {
    "id": "2cf40ced"
   },
   "outputs": [],
   "source": [
    "##TODO do the same with SBERT embeddings"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
