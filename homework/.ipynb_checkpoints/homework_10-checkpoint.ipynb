{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "congressional-subsection",
   "metadata": {},
   "source": [
    "# HW10: A Simple Chatbot using GPT2\n",
    "\n",
    "Remember that these homework work as a completion grade. **You can <span style=\"color:red\">not</span> skip one section this homework.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prescribed-billion",
   "metadata": {},
   "source": [
    "**Training a Chatbot**\n",
    "\n",
    "In this exercise, we are going to train a simple chatbot based on DistilGPT2. Find an overview of the GPT2 architecture in hugggingface [here](https://huggingface.co/transformers/model_doc/gpt2.html). We will use the [CCPE data](https://www.aclweb.org/anthology/W19-5941.pdf) (no need to read the paper for this exercise, we provide data loading utilties). The dataset offers exciting possibilities to train sophisticated chatbots, however we only explore a very simple version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "saving-bangkok",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clone the github repo\n",
    "!git clone https://github.com/google-research-datasets/ccpe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "precise-silly",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "\n",
    "def load_data():\n",
    "    with open(\"ccpe/data.json\") as f:\n",
    "        data = json.load(f)\n",
    "    conversations = []\n",
    "    for conversation in data:\n",
    "        for i, item in enumerate(conversation[\"utterances\"]):\n",
    "            text = item[\"text\"]\n",
    "            if i == 0:\n",
    "                # nothing todo\n",
    "                pass\n",
    "            else:\n",
    "                conversations.append((last_text, text))\n",
    "            last_text = text\n",
    "    return conversations     \n",
    "data = load_data()\n",
    "\n",
    "data = np.reshape(data[:-13], (-1, 16, 2))\n",
    "print (len(data))\n",
    "print (data[0][:5])\n",
    "\n",
    "\n",
    "# please note how we arrange the data in pairs of (previous_sentence, current_sentence)\n",
    "# and each batch contains 16 such sentence pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "expanded-marina",
   "metadata": {},
   "source": [
    "**The data**\n",
    "As we can see from the data, we only extract sentence pairs and will train models on these pairs in isolation. This is a very simplified version of training a chatbot. \n",
    "\n",
    "**In a more realistic setting**, we would include more conversation history, perhaps have to retrieve additional information from a fact base to generate factually accurate examples (e.g. think about a chatbot which could suggest restaurants in a city and needs to have a list available of all restaurants in that city). We would probably also encode the speaker and the chatbot, and guess the speech act to give the desired response (e.g. if a speaker just wants to do small talk, we would guess this and reply accordingly. If we guess he is asking for factual information, we should structure our response very differently. However, in this exercise we stick to our very simplified version.\n",
    "\n",
    "We have already prepared the data such that it comes in batches of 32 examples each.\n",
    "\n",
    "To train a chatbot, we need data, a tokenizer, a model and an optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pretty-diagram",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "model_name = \"distilgpt2\"\n",
    "\n",
    "# load the tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name, pad_token_id=0)\n",
    "\n",
    "##because GPT2 tokenizers do not have padding, cls and sep tokens, we have to add these ourselves\n",
    "##we won't need the # character, so this will be the pad token\n",
    "tokenizer.add_special_tokens({'pad_token': '#'})\n",
    "tokenizer.add_special_tokens({'cls_token': 'bos'})\n",
    "tokenizer.add_special_tokens({'sep_token': 'bos'})\n",
    "\n",
    "##TODO load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "blessed-activation",
   "metadata": {},
   "outputs": [],
   "source": [
    "##let's have a look what we generate before fine-tuning our chatbot\n",
    "\n",
    "input_ids = tokenizer.encode(\"Why do you like this kind of movies?\", return_tensors='pt')\n",
    "\n",
    "sample_output = model.generate(\n",
    "    input_ids, \n",
    "    do_sample=True, \n",
    "    max_length=50, \n",
    "    top_p=0.92, \n",
    ")\n",
    "\n",
    "tokenizer.decode(sample_output[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "colored-thanksgiving",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import Adam\n",
    "##TODO implement an optimizer with learning_rate = 2e-5 for all parameters\n",
    "\n",
    "learning_rate = 2e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "refined-produce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "##TODO train the model\n",
    "num_epochs = 1\n",
    "\n",
    "##on my laptop, training on all 716 batches takes roughly one hour, so we just train for 100 steps \n",
    "max_steps = 100\n",
    "data = data[:max_steps]\n",
    "\n",
    "for i, batch in enumerate(tqdm(data)):\n",
    "    ##TODO prepare model input\n",
    "    #In the textual entailment example in the notebook, we encode\n",
    "    #[CLS-token]premise[SEP-token]hypothesis[SEP-TOKEN]\n",
    "    #Here, we would like to encode \n",
    "    #[CLS-token]previous_sentence[Sep-token]current_sentence[SEP-TOKEN]\n",
    "    \n",
    "    ##Compute a forward step (the labels are simply the input_ids)\n",
    "    #since gpt-2 reads from left to right, it will predict the label at each timestep without having access to that token's information during training\n",
    "    \n",
    "    ##Compute a backward step\n",
    "    \n",
    "    ##Perform an optimzer step\n",
    "\n",
    "    ##Clear gradients of the optimizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bacterial-driving",
   "metadata": {},
   "outputs": [],
   "source": [
    "##let's have a look what we generate after fine-tuning our chatbot\n",
    "\n",
    "input_ids = tokenizer.encode(\"Why do you like this kind of movies?\", return_tensors='pt')\n",
    "\n",
    "sample_output = model.generate(\n",
    "    input_ids, \n",
    "    do_sample=True, \n",
    "    max_length=50, \n",
    "    top_p=0.92, \n",
    ")\n",
    "\n",
    "tokenizer.decode(sample_output[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
