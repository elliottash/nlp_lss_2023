{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ed81bdc6",
   "metadata": {},
   "source": [
    "# HW12: Scientific Claim Verification\n",
    "\n",
    "**<span style=\"color:red\">Important Instructions, read carefully!</span>** \n",
    "\n",
    "* Remember that these homework work as a completion grade. The homework is structured in two parts: (1) Information Retrieval from a fact base and (2) claim verification. In case you already submitted 9 homeworks and want to only submit a 10th notebook, it is fine to just do the first part. \n",
    "\n",
    "* In this notebook, we will build an automated claim verification system for scientific claims based on the [SciFact dataset](https://arxiv.org/abs/2004.14974). \n",
    "\n",
    "* In case you need additional computational resources (GPUs), please get in touch. It is possible to solve the homework (on a downsampled dataset) without these. If you want to build really cool systems, you probably want to use the whole dataset and train models which require compute not feasible on your local machine -- get in touch.\n",
    "\n",
    "* Next, the best models to date on this dataset perform poorly. If you build a system yielding competitive scores, it is possible to do follow-up work! There exists a [global leaderboard](https://leaderboard.allenai.org/scifact/submissions/public) where you can submit test set results if you like. The [baseline system](https://arxiv.org/abs/2004.14974) achieves around 40% F1, we have built a system in January and obtain 55% F1, the [current state of the art](https://arxiv.org/pdf/2010.11930.pdf) is at around 65% F1 -- there exists ample room for improvements.\n",
    "\n",
    "* You can find the github repo for SciFact with additional information (and possibly example code which could be usueful for solving this exercise) [here](https://github.com/allenai/scifact).\n",
    "\n",
    "* For the first part of the assignment, we don't expect you to train your own transformer model, but you obviously can. For the second part of the assignment, we expect you to train your own textual entailment model.\n",
    "\n",
    "* Lastly, We don't expect you to have a competitive system at the end of this homework, anything works as long as you have put in some effort.\n",
    "\n",
    "**All instructions provided can be substituted by your own ideas and only serve as a rough guideline for how to tackle the task!**\n",
    "\n",
    "* If you want, it is also possible to build a similar system for question answering (e.g. [SQuAD](https://rajpurkar.github.io/SQuAD-explorer/explore/1.1/dev/)), automated fact checking (e.g. [Climate-FEVER](https://www.sustainablefinance.uzh.ch/en/research/climate-fever.html), [FEVER](https://fever.ai/)) or other involved NLP tasks completely freestyle on a dataset of your choice. If so, please get in touch with a suggestion.\n",
    "\n",
    "* You are allowed to use any tools and help you can find online to tackle this task."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b653efbf",
   "metadata": {},
   "source": [
    "**An Example from SciFact**\n",
    "\n",
    "Consider the claim *\"Consumption of whole fruits increases the risk of type 2 diabetes.\"*\n",
    "\n",
    "We are given a fact base of 5000 abstracts and have to find evidence from the fact base which either supports or refutes the claim. In this example, the goal is to find the following sentence from the corpus:\n",
    "\n",
    "*'Greater consumption of specific whole fruits, particularly blueberries, grapes, and apples, is significantly associated with a lower risk of type 2 diabetes, whereas greater consumption of fruit juice is associated with a higher risk.'*\n",
    "\n",
    "It is easy to see that this sentence contradicts the claim. The goal of the task is to return all sentences in the fact base which contradict or support a claim -- with the corresponding label. In this case, we would return \n",
    "\"evidence\": {\"1974176\": [{\"sentences\": [11], \"label\": \"CONTRADICT\"}\n",
    "where \"1974176\" is the doc_id of the abstract we found the evidence in, and it is the 11th sentence in that abstract."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e98f24e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtain the data\n",
    "name=\"https://scifact.s3-us-west-2.amazonaws.com/release/latest/data.tar.gz\"\n",
    "!wget $name\n",
    "\n",
    "!tar -xvf data.tar.gz\n",
    "!rm data.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d9d8df4",
   "metadata": {},
   "source": [
    "**Part 1 of the Assignment: Information Retrieval**\n",
    "\n",
    "In this section, we will build a document retrieval system which takes as input a claim and returns a number of candidate abstracts which are similar to the claim. Commonly, we start with a recall-oriented system which returns abstracts likely to contain evidence sentences. Then, we follow up with a more advanced model which selects only relevant sentences from the retrieved abstracts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94a32581",
   "metadata": {},
   "outputs": [],
   "source": [
    "# install some helper utils\n",
    "!pip install jsonlines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6ac4464",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jsonlines\n",
    "\n",
    "# load the corpus (the fact base with the abstracts)\n",
    "corpus = {str(doc['doc_id']): doc for doc in jsonlines.open(\"data/corpus.jsonl\")}\n",
    "\n",
    "# load the claims\n",
    "\n",
    "# if you don't want to work with GPUs , you probably want to only consider 200/50 train/dev examples\n",
    "# otherwise, this exercise might take too long\n",
    "\n",
    "cpu_only = True\n",
    "if cpu_only:\n",
    "    claims_train = [claim for claim in jsonlines.open(\"data/claims_train.jsonl\") if claim[\"evidence\"]][:200]\n",
    "    claims_dev = [claim for claim in jsonlines.open(\"data/claims_dev.jsonl\") if claim[\"evidence\"]][:50]\n",
    "else:\n",
    "    claims_train = [claim for claim in jsonlines.open(\"data/claims_train.jsonl\") if claim[\"evidence\"]][:200]\n",
    "    claims_dev = [claim for claim in jsonlines.open(\"data/claims_dev.jsonl\") if claim[\"evidence\"]][:50]\n",
    "\n",
    "print (len(claims_train))\n",
    "print (len(claims_dev))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8b174d8",
   "metadata": {},
   "source": [
    "**Inspect the data**\n",
    "\n",
    "Let's have a look at the corpus first. We see that every abstract has a unique doc_id, the title of the paper, the abstract (sentences are already tokenized) and a flag \"structured\" which is not relevant for this task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcea28e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"number of documents in the corpus\", len(corpus))\n",
    "print (corpus[\"1974176\"])\n",
    "print (corpus[\"1974176\"].keys()) \n",
    "# dict_keys(['doc_id', 'title', 'abstract', 'structured'])\n",
    "# abstract is a list of sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa394ae5",
   "metadata": {},
   "source": [
    "Next, we look at an example claim in more detail. We find that it has a unique id, the claim as a string and annotated evidence. The evidence is a dictionairy where each key points to the abstract in the corpus. The values are a list where each entry contains the sentence number in the corresponding abstract and a label whether this sentence contradicts or supports the claim."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8eef67a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(claims_train[0])\n",
    "print(claims_train[0].keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44364397",
   "metadata": {},
   "source": [
    "**Random Baseline for Abstract Retrieval**\n",
    "\n",
    "As mentioned before, we need a system which retrieves abstracts from the corpus. Some ideas of how to tackle this include \n",
    "* create document embeddings via TF-IDF, SBERT, universal sentence encoder or any embedding technique you like. Embedd each claim and each abstract. Then find the closest abstracts for each claim\n",
    "* use BM25 for document retrieval\n",
    "* do something else which works\n",
    "\n",
    "We provide a random baseline and evaluate recall for this method. Not surprisingly, this does not work well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "864091c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# random baseline\n",
    "import random\n",
    "def retrieve(claim, corpus, k):\n",
    "    return random.sample(corpus.keys(), k=k)\n",
    "\n",
    "retrieved_documents = []\n",
    "for k in (3,5,10):\n",
    "    for claim in claims_train:\n",
    "        result = retrieve(claim[\"claim\"], corpus, k)\n",
    "        claim[\"doc_ids\"] = result\n",
    "\n",
    "    for claim in claims_dev:\n",
    "        result = retrieve(claim[\"claim\"], corpus, k)\n",
    "        claim[\"doc_ids\"] = result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa32ed94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate\n",
    "def evaluate(claims):\n",
    "    TP, FP, FN = 0, 0, 0\n",
    "    for claim in claims:\n",
    "        # relevant abstracts\n",
    "        if claim[\"evidence\"]:\n",
    "            true_abstracts = set(claim[\"evidence\"].keys())\n",
    "            retrieved_abstracts = set(claim[\"doc_ids\"])\n",
    "            TP += len(true_abstracts.intersection(retrieved_abstracts))\n",
    "            FN += len(true_abstracts.difference(retrieved_abstracts))\n",
    "            FP += len(retrieved_abstracts.difference(true_abstracts))\n",
    "        else:\n",
    "            FP += len(claim[\"doc_ids\"])\n",
    "    try:\n",
    "        pr = TP / (TP + FP)\n",
    "        rc = TP / (TP + FN)\n",
    "        f1 = 2 * pr * rc / (pr + rc)\n",
    "    except ZeroDivisionError:\n",
    "        pr, rc, f1 = 0,0,0\n",
    "    print (\"precision\",pr, \"recall\",rc, \"f1\",f1)\n",
    "\n",
    "print (\"train claims\")\n",
    "evaluate(claims_train)\n",
    "print (\"dev claim\")\n",
    "evaluate(claims_dev)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16b9de74",
   "metadata": {},
   "outputs": [],
   "source": [
    "##TODO create your own abstract retrieval system\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2db2f441",
   "metadata": {},
   "outputs": [],
   "source": [
    "##TODO evaluate your system. \n",
    "# If it operates on document level, we suggest to evaluate your system with k=3, k=5, k=10 retrieved documents\n",
    "\n",
    "# else, evaluate it using some reasonable method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10c45f9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save your results, we suggest for k=3, which makes the rest of this exercise less time consuming\n",
    "\n",
    "k=3\n",
    "for claim in claims_train:\n",
    "    result = retrieve(claim[\"claim\"], corpus, k)\n",
    "    claim[\"doc_ids\"] = result\n",
    "\n",
    "for claim in claims_dev:\n",
    "    result = retrieve(claim[\"claim\"], corpus, k)\n",
    "    claim[\"doc_ids\"] = result\n",
    "    \n",
    "import json\n",
    "with open(\"data/claims_train_with_retrieved_documents.jsonl\", \"w\") as outfile:\n",
    "    for claim in claims_train:\n",
    "        json.dump(claim, outfile)\n",
    "        outfile.write(\"\\n\")\n",
    "        \n",
    "with open(\"data/claims_dev_with_retrieved_documents.jsonl\", \"w\") as outfile:\n",
    "    for claim in claims_dev:\n",
    "        json.dump(claim, outfile)\n",
    "        outfile.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4a8ad7a",
   "metadata": {},
   "source": [
    "**Sentence Retrieval**\n",
    "\n",
    "Now, we have candidate documents for every claim. As we have seen before, the precision achieved is not very convincing. So, we train a second module which takes a claim and a sentence as input and decides whether this sentence is possible evidence which supports or verifies the claim. This is just another pairwise sentence classification task and is usually tackled as a binary classification.\n",
    "\n",
    "* If you want to train your own model on CPU, we suggest to use [distilbert](https://huggingface.co/distilroberta-base) (which took me 20 minutes to fine-tune for one epoch on CPU). If you have access to GPUs, there's a variety of models to choose from, e.g. [here](https://huggingface.co/transformers/pretrained_models.html) or [here](https://huggingface.co/models).\n",
    "\n",
    "* We also provide a model [here](https://www.dropbox.com/s/mh3lrg3z626d0xw/scibert_model.zip?dl=0) which is a BertForSequenceClassification checkpoint fine-tuned from [SciBERT](https://huggingface.co/allenai/scibert_scivocab_uncased) which you could use in this task. The model is trained to predict class 1 for annotated evidence sentences and class 0 for randomly sampled negative examples. You can download this model with \n",
    "\n",
    "* wget https://www.dropbox.com/s/mh3lrg3z626d0xw/scibert_model.zip?dl=0\n",
    "\n",
    "* You can use any other model/method you like if you think it works reasonably well on this specific task\n",
    "\n",
    "* We again provide a random baseline for demonstration purposes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a1b9251",
   "metadata": {},
   "outputs": [],
   "source": [
    "# random baseline\n",
    "\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "for claim in tqdm(claims_dev):\n",
    "    doc_ids = claim[\"doc_ids\"]\n",
    "    predicted_evidence = {}\n",
    "    for doc_id in doc_ids:\n",
    "        sentences = corpus[doc_id][\"abstract\"]\n",
    "        predictions = np.random.normal(loc=-1, size=len(sentences))\n",
    "        predicted_sentences = [i for i,j in enumerate(predictions) if j > 0]\n",
    "        if predicted_sentences:\n",
    "            predicted_evidence[doc_id] = {\"sentences\": predicted_sentences}\n",
    "    claim[\"predicted_evidence\"] = predicted_evidence\n",
    "\n",
    "with open(\"data/claims_dev_with_predicted_sentences.jsonl\", \"w\") as outfile:\n",
    "    for claim in claims_dev:\n",
    "        json.dump(claim, outfile)\n",
    "        outfile.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f83f890",
   "metadata": {},
   "outputs": [],
   "source": [
    "##TODO for every claim in the development set, and for every sentence in each retrieved abstract\n",
    "#predict whether it is evidence or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be0bac29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# some utils to evalaute this using the official metrics for SciFact\n",
    "from collections import Counter\n",
    "def safe_divide(num, denom):\n",
    "    if denom == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return num / denom\n",
    "\n",
    "def compute_f1(counts, difficulty=None):\n",
    "    correct_key = \"correct\" if difficulty is None else f\"correct_{difficulty}\"\n",
    "    precision = safe_divide(counts[correct_key], counts[\"retrieved\"])\n",
    "    recall = safe_divide(counts[correct_key], counts[\"relevant\"])\n",
    "    f1 = safe_divide(2 * precision * recall, precision + recall)\n",
    "    return {\"precision\": precision, \"recall\": recall, \"f1\": f1}\n",
    "\n",
    "def is_correct(pred_sentence, pred_sentences, gold_sets):\n",
    "    \"\"\"\n",
    "    A predicted sentence is correctly identified if it is part of a gold\n",
    "    rationale, and all other sentences in the gold rationale are also\n",
    "    predicted rationale sentences.\n",
    "    \"\"\"\n",
    "    for gold_set in gold_sets:\n",
    "        gold_sents = gold_set[\"sentences\"]\n",
    "        if pred_sentence in gold_sents:\n",
    "            if all([x in pred_sentences for x in gold_sents]):\n",
    "                return True\n",
    "            else:\n",
    "                return False\n",
    "\n",
    "    return False\n",
    "\n",
    "\n",
    "def evaluate_sentence_retrieval(dataset, rationale_selection):\n",
    "    counts = Counter()\n",
    "    for data, retrieval in zip(dataset, rationale_selection):\n",
    "        assert data['id'] == retrieval['id']\n",
    "\n",
    "        # Count all the gold evidence sentences.\n",
    "        for doc_key, gold_rationales in data[\"evidence\"].items():\n",
    "            for entry in gold_rationales:\n",
    "                counts[\"relevant\"] += len(entry[\"sentences\"])\n",
    "\n",
    "        claim_id = retrieval['id']\n",
    "        for doc_id, pred_sentences in retrieval['predicted_evidence'].items():\n",
    "            true_evidence_sets = data['evidence'].get(doc_id) or []\n",
    "            for pred_sentence in pred_sentences:\n",
    "                counts[\"retrieved\"] += 1\n",
    "                if is_correct(pred_sentence, pred_sentences, true_evidence_sets):\n",
    "                    counts[\"correct\"] += 1\n",
    "    f1 = compute_f1(counts)\n",
    "    print(f1)\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0f48511",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate \n",
    "evaluate_sentence_retrieval(claims_dev, claims_dev)\n",
    "# and we find that our random baseline behaves poorly :(\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd15ec5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "##TODO evaluate your predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed9a4a3a",
   "metadata": {},
   "source": [
    "**Part 2 of the Assignment: Claim Verification**\n",
    "\n",
    "To recap: For every claim, we have retrieved possible evidence sentence. Now, we want to determine whether these sentences support or contradict a claim. Usually, this is handled via textual entailment; if the evidence entails the claim, it is supported (and else, it is contradicted). For this task, you should train your own model, we propose to start from a [distilbert checkpoint](https://huggingface.co/typeform/distilbert-base-uncased-mnli) which has been pre-trained on MNLI. \n",
    "\n",
    "Again, we provide a random baseline and evaluate this baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf86ad21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# random baseline\n",
    "\n",
    "id2label = {0:\"SUPPORT\", 1: \"NOT_ENOUGH_INFO\", 2:\"CONTRADICT\"}\n",
    "for claim in claims_dev:\n",
    "    predicted_evidence = claim[\"predicted_evidence\"]\n",
    "    labels = {}\n",
    "    for doc_id, sentence_ids in predicted_evidence.items():\n",
    "        abstract = corpus[doc_id][\"abstract\"]\n",
    "        sentences = \" \".join(abstract[i] for i in sentence_ids[\"sentences\"])\n",
    "        label = id2label[np.random.choice([0,1,2])]\n",
    "        # if we predict neutral, we just ignore these evidence sentences\n",
    "        labels[doc_id] = {\"label\": label}\n",
    "    claim[\"labels\"] = labels\n",
    "            \n",
    "        \n",
    "claims_dev[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "657c6737",
   "metadata": {},
   "outputs": [],
   "source": [
    "##TODO create an appropriate dataset to fine-tune your model \n",
    "# (input to your model should be [CLS] claim [SEP] evidence_sentence [SEP]\n",
    "# it might be required to sample some evidence sentences which are not annotated and act as \"neutral\" or \n",
    "# \"NOT_ENOUGH_INFO\" examples\n",
    "# (hint: labels in mnli are: LABELS = {'CONTRADICT': 0, 'NOT_ENOUGH_INFO': 1, 'SUPPORT': 2}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c58d7bd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# some utils to evalaute this using the official metrics for SciFact\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, confusion_matrix\n",
    "def evaluate_labels(dataset, label_prediction):\n",
    "    LABELS = {'CONTRADICT': 0, 'NOT_ENOUGH_INFO': 1, 'SUPPORT': 2}\n",
    "    pred_labels = []\n",
    "    true_labels = []\n",
    "\n",
    "    for data, prediction in zip(dataset, label_prediction):\n",
    "        assert data['id'] == prediction['id']\n",
    "\n",
    "        if not prediction['labels']:\n",
    "            continue\n",
    "\n",
    "        claim_id = data['id']\n",
    "        for doc_id, pred in prediction['labels'].items():\n",
    "            pred_label = pred['label']\n",
    "            true_label = {es['label'] for es in data['evidence'].get(doc_id) or []}\n",
    "            assert len(true_label) <= 1, 'Currently support only one label per doc'\n",
    "            true_label = next(iter(true_label)) if true_label else 'NOT_ENOUGH_INFO'\n",
    "            pred_labels.append(LABELS[pred_label])\n",
    "            true_labels.append(LABELS[true_label])\n",
    "\n",
    "    print(f'Accuracy           {round(sum([pred_labels[i] == true_labels[i] for i in range(len(pred_labels))]) / len(pred_labels), 4)}')\n",
    "    print(f'Macro F1:          {f1_score(true_labels, pred_labels, average=\"macro\").round(4)}')\n",
    "    print(f'Macro F1 w/o NEI:  {f1_score(true_labels, pred_labels, average=\"macro\", labels=[0, 2]).round(4)}')\n",
    "    print()\n",
    "    print('                   [C      N      S     ]')\n",
    "    print(f'F1:                {f1_score(true_labels, pred_labels, average=None).round(4)}')\n",
    "    print(f'Precision:         {precision_score(true_labels, pred_labels, average=None).round(4)}')\n",
    "    print(f'Recall:            {recall_score(true_labels, pred_labels, average=None).round(4)}')\n",
    "    print()\n",
    "    print('Confusion Matrix:')\n",
    "    print(confusion_matrix(true_labels, pred_labels))\n",
    "evaluate_labels(claims_dev, claims_dev)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb98ade8",
   "metadata": {},
   "outputs": [],
   "source": [
    "##TODO evaluate your own predictions"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
